\documentclass{article}

\title{Baseball Research Log}
\author{Chris Comiskey}
\date{Winter 2017}

\usepackage{natbib}
\bibliographystyle{unsrtnat}

\usepackage{fullpage}
\usepackage{ulem}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{mathtools}
\usepackage{float}
\usepackage{bbm}
\usepackage{mathrsfs}

\usepackage{listings}


\begin{document}

\maketitle{}

\section*{Week of December 5, 2016} % ========================
\begin{itemize}
\item An option:
      \begin{itemize}
      \item Started with $\pmb{\beta} \sim \text{N}(\pmb{0}, 5 \text{{\bf I}}_{6})$ non-informative prior 
      \item ...Define $\pmb{\theta} = \pmb{R \beta}$
      \item ...giving Var$(\pmb{\theta}) = \pmb{R}\text{Var}(\pmb{\beta})\pmb{R}'$
      \item Then use prior distribution $\pmb{\theta} \sim \text{N}(\pmb{R \hat{\beta}}, 5 *\pmb{R R'})$ (...but diagonal only)
      \end{itemize}
\item \verb|Hitter6.stan|. Used $\pmb{R}$ and polar $\hat{\pmb{\beta}}_{\text{GLM}}$ to estimate $\text{E}[\pmb{\theta}]$ and $\text{Var}[\pmb{\theta}]$, as in previous bullet point.
\begin{verbatim}
data {                             
  int<lower=0> N;                 // Number of observations
  int<lower=0> p;                 // number of predictors
.
.
.
      Z ~ normal(0, 1);  // Each element is N(0,1)
      Z_mod = Sigma * Z; // (Cov matrix Cholesky)*MVN(0,1)
      hit ~ bernoulli_logit(beta0 + Q*theta + Z_mod);
}
\end{verbatim}
\item Reading \cite{Finley2007}
\item Also looking at Andy's notes from NCAR. 
\item Reading slides titled ``functional-casestudy.pdf''
\item Idea: could define $\beta$ in \verb|Hitter6.stan| as $\hat{\beta}_{\text{GLM}}$, to get idea of length-scale ballpark.
\item Alarming question: is Z a parameter?
\item Ran \verb|Hitter5.stan| overnight, for N = 2000. Only made it through 350 samples. Prohibitive.
\item Gotta try sampling loop. For i in 1:100, sample 200, fit with \verb|Hitter5.stan|, then compile all those draws. Would that work/help?
\item Reading \cite{Neal2011} to learn about Hamiltonian MCMC (HMC) algorithm, which Stan uses. Hamiltonian Dynamics; Mr. Meyers would be proud.
\item Wikipedia:  ``In mathematics, the {\bf gradient} is a generalization of the usual concept of derivative to functions of several variables.'' 
\item What if I used my variable resolution grid to estimate variance parameters? 
  \begin{itemize}
  \item Using the usual box centers reduction would be shoddy because many boxes would have very little data, and thus misrepresent Cov(Box1, Box2). 
  \item For example, $\hat{p}_{\text{Box1}} = 1$ for one out of one, against $\hat{p}_{\text{Box2}} = 0.12$ for 12 out of 100, misrepresents Cov(Box1, Box2). 
  \item Using my variable resolution grid would offer advantages, by eliminating $\hat{p}_{\text{Box1}} = 1$ scenarios.
  \end{itemize}
\end{itemize}

\subsection*{Meeting}
\begin{itemize}
\item Alix was worried, because I wanted to meet {\it this week}, that I had found a job and was going to leave early! She said she has had other students do the same, and it is very hard to finish in that fashion.
\item Alix read my paper, made some comments, said it was good. She thinks at the end there should be a section of sorts that helps the (baseball) reader understand and interpret the new map, rather than than just understanding the algorithm. What do the box sizes tell us? What does the particular spatial variation in box size tell us? 
\item Alix is thinking space might be a dual stopping criteria. For example, can hitters even distinguish pitches in two adjacent very small boxes? They probably look like the identical pitch, so why assign separate batting averages.
\item ``Banish from your mind'' the question of whether or not this is PhD level research. She said there is {\bf no way anyone below the PhD level could make their way through} (i) Finley's papers, (ii) the  QR decomposition of $\pmb{X \beta}$ (linear model theory) in my logistic regression model (GLM) and the resultant change in the prior distributions (Bayesian-LMT), or (iii) MCMC with Hamiltonian dynamics, etc (just the stuff we talked about today). And, she said, it's very cool we're applying it to something totally different (other than forestry, etc), baseball, where it totally applies. Bottom line: {\bf Alix is sure.} She's my primary adviser. ``God,'' as Bruce put it. Her opinion carries all the weight. The jury starts and stops with her. She believes it, I get a PhD.
\item Can always contact Andy and/or Malcom is needbe
\item Need to try leaving $\theta \sim N(\pmb{0}, 5*\pmb{R*R^{T}})$. In other words, shoul be fine to leave the mean uninformative if adjust variance.
\item Okay that $\theta$s correlated according to $Var(\theta) = 5*\pmb{R*R^{T}}$ because coefficient estimates always are, even in simple linear regression.
\item My idea of using variable resolution grid to change grid-reduced estimates of correlation parameters is worth coming back to. She wasn't sure if the base method existed (or not) to improvise upon.
\end{itemize}

\subsection*{Carrying on...}
\begin{itemize}
\item As mentioned above. Correct adjusted prior variances on $\hat{\theta}$, with mean zero. \verb|Hitter5WOSC|
\begin{verbatim}
> print(fit_hitterWOSC, pars=c("beta0","beta"), digits = 3)
Inference for Stan model: Hitter5WOSC.
3 chains, each with iter=500; warmup=250; thin=1; 
post-warmup draws per chain=250, total post-warmup draws=750.

          mean se_mean    sd   2.5%    25%    50%    75%  97.5% n_eff  Rhat
beta0   -4.068   0.055 0.682 -5.475 -4.516 -4.066 -3.627 -2.800   155 1.015
beta[1]  1.168   0.038 0.487  0.219  0.847  1.150  1.473  2.228   168 1.014
beta[2] -1.929   0.127 1.874 -5.803 -3.199 -1.812 -0.535  1.314   217 1.008
beta[3] -0.316   0.006 0.086 -0.494 -0.369 -0.315 -0.261 -0.149   236 1.009
beta[4] -3.978   0.064 1.093 -6.225 -4.742 -3.910 -3.187 -2.059   290 1.006
beta[5] -1.691   0.059 0.887 -3.357 -2.289 -1.722 -1.065  0.106   229 1.008
beta[6] -0.471   0.013 0.212 -0.861 -0.618 -0.483 -0.321 -0.056   282 1.006
\end{verbatim}
As they should be! They match the plane Jane GLM fit without spatial correlation random effect. 
\item \verb|Hitter5WOSC.stan|
\begin{verbatim}
data {                             
  int<lower=0> N;                 // Number of observations
  int<lower=0> p;                 // number of predictors
  matrix[N,p] Q;                  // QR decomp - Q
  matrix[p,p] R;                  // QR decomp - R
  int<lower=0, upper=1> hit[N];   // 0/1 outcomes; array of integers
  vector[p] theta_SDs;            // theta prior SDs
}
transformed data{
  matrix[p,p] R_inv;
  R_inv = inverse(R);
}
parameters {                
  real beta0;                       // intercept 
  vector[p] theta;
}
transformed parameters {
      vector[p] beta;
      beta = R_inv*theta;
}
model {  
      beta0 ~ normal(0,5);
      theta[1] ~ normal(0, theta_SDs[1]);
      theta[2] ~ normal(0, theta_SDs[2]);
      theta[3] ~ normal(0, theta_SDs[3]);
      theta[4] ~ normal(0, theta_SDs[4]);
      theta[5] ~ normal(0, theta_SDs[5]);
      theta[6] ~ normal(0, theta_SDs[6]);
      hit ~ bernoulli_logit(beta0 + Q*theta);
}
\end{verbatim}

\item {\bf Ask Alix:} Try sampling loop... For i in 1:100, sample 200, fit with \verb|Hitter5.stan|, then compile all those draws. Would that work/help?
\end{itemize}

\section*{Week of January 9, 2017} % ========================
\begin{itemize}
\item Where was I?
  \begin{enumerate}
  \item STAN needs to go faster; enter Andrew Finley
  \item Andrew Finley, reduce dimensionality of correlation
  \item Paper; Alix edits/feedback to look at
  \item Expand picture effort to include 3D representation of CIs
  \item Hamilton equations
  \end{enumerate}
\item Confirmed: Using 9177 obs, QR decomposition, and QR inflated variances, but leaving out spatial correlation, all incorporated into \verb|Hitter5WOSC.stan| yields same estimates as \verb|glm()|. So stan code is working, and QR decomposition is working. As of now, running the same procedure WITH spatial correlation is prohibitively slow.

\end{itemize}

\subsection*{MCMC Using Hamiltonian Dynamics \citep{Neal2011}}
\begin{itemize}
\item Paraphrased: MCMC originated with (Metropolis et al. 1953) to simulate states for molecules... another approach (Alder and Wainwright, 1959) formalized Newton’s laws of motion as Hamiltonian dynamics.
\item Duane et al. 1987 paper united approaches as Hybrid MC, or Hamiltonian MC $\rightarrow$ HMC for lattice field theory quantum dynamic simulations. 
\item Statistical use began in 1996 with neural network models, other applications followed.
\item Differential equations and ``leapfrog'' scheme, elementary mathematics

\end{itemize}
Steps.
      \begin{enumerate}
      \item ``Define a Hamiltonian function in terms of the probability distribution we wish to sample from.''
      \item Interested in ``position'' variables. Introduce auxiliary ``momentum'' variables (typically independent Gaussian distributions)
      \item The HMC method alternates:
        \begin{enumerate}
        \item Simple updates for these momentum variables 
        \item Metropolis updates - computing Hamiltonian dynamic trajectory for new proposal state, leapfrog implementation method, distant proposals w/ high acceptance probability; bypass slow random walk proposal distribution
        \end{enumerate}
      \end{enumerate}
\begin{itemize}
\item Physical interpretation - frictionless puck on varying height surface. Puck at {\bf position} $q$ with {\bf momentum} $p$ and mass $m$, {\bf potential energy} $U(q)$, {\bf kinetic energy} $K(p) = |p|^{2}/(2m)$.
\item Wikipedia: ``In statistics, especially in Bayesian statistics, the {\bf kernel} of a probability density function (pdf) or probability mass function (pmf) is the form of the pdf or pmf in which any factors that are not functions of any of the variables in the domain are omitted.''
\end{itemize}

\subsubsection{Equations}
\begin{itemize}
\item q = position, U(q) = potential energy 
      \begin{itemize}
      \item \pmb{q} = variables of interest
      \item let $U(q) = -\text{log}f_{q}(q)$
      \end{itemize}
\item p = momentum, K(p) = kinetic energy
      \begin{itemize}
      \item Introduce artificially, same dimension as \pmb{q}
      \end{itemize}
\item Partial derivatives determine change over time. For $i = 1,\dots, d$:
$$ \frac{d q_{i}}{dt} = \frac{\partial H}{\partial p_{i}}$$
$$ \frac{d p_{i}}{dt} = -\frac{\partial H}{\partial q_{i}}$$

\item $H(q,p) = U(q) + K(p) = -\text{log}f_{q}(q) + p^{T}\pmb{M}^{-1}p/2$
    \begin{itemize}
    \item Total = Potential + Kinetic
    \item $U(q) = -\text{log}f_{q}(q)$
    \item $K(p) = -\text{log }f_{p}(p) = p^{T}\pmb{M}^{-1}p/2$, (using p $\sim$ N(0,\pmb{M}))
    \end{itemize}
\item Rewrite:
$$ \frac{d q_{i}}{dt} = \frac{\partial H}{\partial p_{i}} = [\pmb{M}^{-1}p]_{i}$$
$$ \frac{d p_{i}}{dt} = -\frac{\partial H}{\partial q_{i}} =- \frac {\partial U}{\partial q_{i}} $$
\item Solution: q(t) = ?, p(t) = ?
\end{itemize}

\section*{Week of January 17, 2017} % ========================

\subsection*{Hamiltonian Mechanics/Dynamics }
\begin{itemize}
\item Leapfrog method = for calculating new position (q) and momentum (p) through tiny time steps
  \begin{itemize}
  \item for {\bf discretizing Hamilton equations}
  \item akin to Taylor Series appoximations
  \item Postion (q) (or momentum (p)) at $t_{0}$ plus time step times rate of change of position (q) (momentum (p)) variable at $t_{0}$
  \item Leapfrom Method does half step for momentum (p), full step for postion (q), other half step for momentum (p). Damn good.

  \end{itemize}
\item {\bf Statistical ensemble} - probability distribution for the state of a system (set of all possible copies)
\item Short version: randomly sample from K(p) (kinetic, momentum), calculate U(k) (potential, position*) --- that's your Metropolis proposal.
\item {\bf Solve} a differential equation: If $\frac{dq}{dt} = p$, and $\frac{dp}{dt} = q$, then q(t) = ?, and p(t) = ?
\end{itemize}

\subsection*{Pictures}
Consider this presentation of CIs in 3D. \\
\includegraphics[scale=.4]{Images/FightEntropy.jpg} \\
Note the line segments through the surface. \\
Brainstorming.
\begin{itemize}
\item Could do a {\bf {\it shiny} app (!!)} that gives a $1-\alpha$ slider to move \sout{three-dimensionally} through the CI, showing the lower ($\alpha/2$) and upper bound ($1-\alpha/2$) maps on the left (lower) and right (upper) of the point estimate map. The left and right would converge to middle map (point estimate) for 0\% CI. 
\item Could even... {\bf have option for cutoff(!!)} as part of app! ...and px, pz, CI percentile
\item Could have a profile plot attached, with px or pz on x-axis, $\hat{p}$ on y-axis---with colors. \\
    \includegraphics[scale=.2]{Images/PolarGLM_ProfileCI.pdf} 
\item Confusion: In my heat maps, color = $\hat{p}$. In my mental images of  CIs $\hat{p}$ the 3D qualities of height and width represent $\hat{p}$ too. 
\item {\bf shiny} app is the key.
\end{itemize}

\subsection*{Shiny}
  \begin{itemize}
  \item See \verb|Shiny.R|
  \item Tutorials: https://shiny.rstudio.com/tutorial/ 
  \end{itemize}
\subsection*{Lesson 1 - Recap}
To create your own Shiny app:
  \begin{enumerate}
  \item Make a directory named for your app.
  \item Save your app’s \verb|server.R| and \verb|ui.R| script inside that directory.
  \item Launch the app with runApp (button) or RStudio’s keyboard shortcuts (Command+Shift+Enter).
  \item Exit the Shiny app by clicking escape.
  \end{enumerate}
  
\subsection*{Lesson 2 - Build a User Interface}
\begin{itemize}
\item create a user-interface with \verb|fluidPage()|, \verb|titlePanel()| and \verb|sidebarLayout()|
          \begin{verbatim}
      # ui.R
      
      shinyUI(fluidPage(
      
        titlePanel("title panel"),
      
        sidebarLayout(
          sidebarPanel("sidebar panel"),
          mainPanel("main panel")
        )
        
      ))
            \end{verbatim}
\item create an HTML element with one of Shiny’s tag functions; i.e. \verb|h2()|, \verb|br()|, etc.
\item set HTML tag attributes in the arguments of each tag function; i.e. \verb|h2("header", align = "center")|
\item add an element (i.e. \verb|img(...)| or \verb|h2(...)|) to your web page by passing it to  \verb|titlePanel|,  \verb|sidebarPanel| or  \verb|mainPanel|.
\item add multiple elements to each panel by separating them with a comma
\item add images by placing your image in a folder labeled  \verb|www| within your Shiny app directory and then calling the  \verb|img| function (\verb|img(src = "bigorb.png", height = 72, width = 72)|)
\end{itemize}

\subsection*{Lesson 3 - Add Control Widgets}
\begin{itemize}
\item Widget - A web element users can interact with.
\item Widgets have help pages; \verb|?actionButton|
\end{itemize}


\subsection*{Meeting}
\begin{itemize}
\item Dissertation
        \begin{itemize}
        \item Chapter 1: I made the Alix feedback changes (Alix happy), I will add Shiny CI presentation stuff to round it out. Doesn't need to be more technical.
        \item Chapter 2 (and this term): \verb|spbayes()|, fit model with random effect.
        \item Chapter 3: Does random effect improve* model? *How do we measure *improve.
        \end{itemize}
\item Need to brush up on ``credible interval,'' the Bayesian paradigm's answer to confidence intervals.
\item I explained my plan for Shiny app approach to CI. They liked the idea. Alix especially.
        \begin{itemize}
        \item Gameplan is to show CI for variable resolution heat map
        \item And for continuous model {\bf without} random effect, but to only briefly introduce the model at this point, and allude to thorough introduction to come in Chapter 2.
        \end{itemize}
\item Charlotte went to RStudio conference. 
        \begin{itemize}
        \item Mango had a table there, promoting their training mostly (she thinks). 
        \item She said they have fully bought into the Hadley Wickham universe of packages. 
        \item Mango now teaches R by starting with \verb|dplyr|, data frames, etc. 
        \item (This is great news for me!) 
        \item She said Mango was one of the companies on her radar if academia didn't work out!
        \end{itemize}
\item Alix asked: ``Is HMC what \verb|spbayes()| uses?'' 
        \begin{itemize}
        \item I don't know, but I don't need to own Hamiltonian dynamics if \verb|spbayes()| doesn't use it. Good point Alix. 
        \end{itemize}
\end{itemize}


\section*{Week of January 23, 2017} % ========================

\subsection*{Lesson 4 - Reactive *Output*}
\begin{itemize}
\item Use an \verb|*Output| (e.g. \verb|textOutput(...)|) function in the \verb|ui.R| script to place reactive objects in your Shiny app
\begin{verbatim}
# ui.R

shinyUI(fluidPage(
  titlePanel("censusVis"),
  
  sidebarLayout(
    sidebarPanel(
      helpText("Create demographic maps with 
               information from the 2010 US Census."),
      
      selectInput("var", 
                  label = "Choose a variable to display",
                  choices = c("Percent White", "Percent Black",
                              "Percent Hispanic", "Percent Asian"),
                  selected = "Percent White"),
      
      sliderInput("range", 
                  label = "Range of interest:",
                  min = 0, max = 100, value = c(0, 100))
      ),
    
    mainPanel(
      textOutput("text1"),  # **Notice "output$text1"
      textOutput("text2")
    )
  )
))
\end{verbatim}
\item Use a \verb|render*| function in the \verb|server.R| script to tell Shiny how to build your objects
\begin{verbatim}
# server.R

shinyServer(function(input, output) {

  output$text1 <- renderText({            # **Notice "output$text1"
    paste("You have selected", input$var) # **Notice "input$var"
    }) 
    
  # ** Notice "output$text2" contains "input$range[1]"  
  output$text2 <- renderText({
    paste("Your range is", input$range[1], "to", input$range[2])
  })
    
})
\end{verbatim}
\item Surround R expressions by braces, {}, in each \verb|render*| function
\item Save your \verb|render*| expressions in the output list, with one entry for each reactive object in your app.
\item Create reactivity by including an input value in a \verb|render*| expression
\end{itemize}

* Workflow seems to me: \verb|ui.R| takes {\bf input} $\longrightarrow$ \verb|server.R| receives {\bf input} and creates {\bf output} $\longrightarrow$ \verb|ui.R| displays {\bf output}.

* Reactivity: connect \verb|input| {\it values} to \verb|output| {\bf objects}.

\subsection*{Lesson 5 - Use R Scripts and Data}
\begin{verbatim}
# server.R

  # A place to put code ***

shinyServer(
  function(input, output){
    
      # Another place to put code ***
    
      output$map <- renderPlot({
    
          # A third place to put code ***
          
      )}
      
    }
  )
\end{verbatim}
\begin{itemize}
\item The \verb|server.R| script is run once, when you launch your app
\item The unnamed function inside \verb|shinyServer| is run once each time a user visits your app
\item The R expressions inside \verb|render*| functions are run many times. Shiny runs them once each time a user changes a widget.
\end{itemize}
How can you use this information?
\begin{itemize}
\item Source scripts, load libraries, and read data sets at the beginning of \verb|server.R| outside of the \verb|shinyServer| function. Shiny will only run this code once, which is all you need to set your server up to run the R expressions contained in \verb|shinyServer|.

\item Define user specific objects inside \verb|shinyServer|’s unnamed function, but outside of any \verb|render*| calls. These would be objects that you think each user will need their own personal copy of. For example, an object that records the user’s session information. This code will be run once per user.

\item Only place code that Shiny must rerun to build an object inside of a \verb|render*| function. Shiny will rerun all of the code in a \verb|render*| chunk each time a user changes a widget mentioned in the chunk. This can be quite often.

\item You should generally avoid placing code inside a render function that does not need to be there. The code will slow down the entire app.
\end{itemize}
Recap
\begin{itemize}
\item You can create more complicated Shiny apps by loading R Scripts, packages, and data sets.
\item The directory that \verb|server.R| appears in will become the working directory of the Shiny app
\item Shiny will run code placed at the start of \verb|server.R|, before \verb|shinyServer|, only once during the life of the app.
\item Shiny will run code placed inside \verb|shinyServer| multiple times, which can slow the app down.
\item You also learned that \verb|switch| is a useful companion to multiple choice Shiny widgets. Use switch to change the values of a widget into R expressions.
\end{itemize}

\subsection*{Lesson 6 - Reactive *Expressions*}
Reactive expressions save their calculated value, and only recalculate when a widget input changes.
\begin{verbatim}
# server.R

library(quantmod)
source("helpers.R")

shinyServer(function(input, output) {

  # Use reactive({.}) expression to create object
  dataInput <- reactive({
    getSymbols(input$symb, src = "yahoo", 
      from = input$dates[1],
      to = input$dates[2],
      auto.assign = FALSE)
  })
  
  # Use reactive object in render*({.}) statment
  output$plot <- renderPlot({    
    chartSeries(dataInput(), theme = chartTheme("white"), 
      type = "line", log.scale = input$log, TA = NULL)
  })
})
\end{verbatim}
``When you click the “Plot y axis on the log scale” widget button, \verb|input$log| will change and \verb|renderPlot| will re-execute. Now
      \begin{enumerate}
      \item \verb|renderPlot| will call \verb|dataInput()|
      \item \verb|dataInput| will check that the "dates" and "symb" widgets have not changed
      \item \verb|dataInput| will return its saved data set of stock prices {\it without re-fetching data from Yahoo}
      \item \verb|renderPlot| will re-draw the chart with the correct axis.
      \end{enumerate}
Shiny will automatically re-build an object if
\begin{itemize}
\item an input value in the objects's \verb|render*| function changes, or
\item a reactive expression in the objects's \verb|render*| function becomes obsolete
\end{itemize}
{\bf Reactive expressions save their results, and will only re-calculate if their input has changed.}
\subsubsection*{Lesson 7}
Share your apps...
\begin{itemize}
\item {\bf Shinyapps.io}

The easiest way to turn your Shiny app into a web page is to use shinyapps.io (http://my.shinyapps.io/), RStudio's hosting service for Shiny apps. 

shinyapps.io lets you upload your app straight from your R session to a server hosted by RStudio. You have complete control over your app including server administration tools. You can find out more about shinyapps.io by visiting shinyapps.io (http://my.shinyapps.io/).
\end{itemize}

\subsection*{Finley, Quest for Gaussian Predictive Process Models}
\begin{itemize}
\item This \citep{Finley2009} looks possibly helpful.
\item Bingo. \citep{Finley2009_2}. 
      \begin{itemize}
      \item Abstract: ``...spatially-varying multinomial logistic regression models to predict forest type groups... spatially-varying impact of predictor variables... onerous computational burdens and we discuss dimension reducing spatial processes...''
      \end{itemize}
\item Maybe \citep{Finley2011}
\item Looks cool. \citep{Guhaniyogi2011}
  \begin{itemize}
  \item Large point referenced datasets occur frequently in the environmental and natural sciences. Use of Bayesian hierarchical spatial models for analyzing these datasets is undermined by onerous computational burdens associated with parameter estimation. Low-rank spatial process models attempt to resolve this problem by projecting spatial effects to a lower-dimensional subspace. This subspace is determined by a judicious choice of ‘knots’ or locations that are fixed a priori. One such representation yields a class of predictive process models (e.g., Banerjee et al., 2008) for spatial and spatial-temporal data. Our contribution here expands upon predictive process models with fixed knots to models that accommodate stochastic modeling of the knots. We view the knots as emerging from a point pattern and investigate how such adaptive specifications can yield more flexible hierarchical frameworks that lead to automated knot selection and substantial computational benefits.
  \end{itemize}
\item \citep{Finley2012}
\item \citep{Eidsvik2012}
  \begin{itemize}
  \item The challenges of estimating hierarchical spatial models to large datasets are addressed. With the increasing availability of geocoded scientific data, hierarchical models involving spatial processes have become a popular method for carrying out spatial inference. Such models are customarily estimated using Markov chain Monte Carlo algorithms that, while immensely flexible, can become prohibitively expensive. In particular, fitting hierarchical spatial models often involves expensive decompositions of dense matrices whose computational complexity increases in cubic order with the number of spatial locations. Such matrix computations are required in each iteration of the Markov chain Monte Carlo algorithm, rendering them infeasible for large spatial datasets. The computational challenges in analyzing large spatial datasets are considered by merging two recent developments. First, the predictive process model is used as a reduced-rank spatial process, to diminish the dimensionality of the model. Then a computational framework is developed for estimating predictive process models using the integrated nested Laplace approximation. The settings where the first stage likelihood is Gaussian or non-Gaussian are discussed. Issues such as predictions and model comparisons are also discussed. Results are presented for synthetic data and several environmental datasets.
  \end{itemize}
\item {\bf spBayes for large univariate and multivariate point-referenced spatio-temporal data models} \citep{Finley2013}
  \begin{itemize}
  \item In this paper we detail the reformulation and rewrite of core functions in the spBayes R package. These efforts have focused on improving computational efficiency, flexibility, and usability for point-referenced data models. Attention is given to algorithm and computing developments that result in improved sampler convergence rate and efficiency by reducing parameter space; decreased sampler run-time by avoiding expensive matrix computations, and; increased scalability to large datasets by implementing a class of predictive process models that attempt to overcome computational hurdles by representing spatial processes in terms of lower-dimensional realizations. Beyond these general computational improvements for existing model functions, we detail new functions for modeling data indexed in both space and time. These new functions implement a class of dynamic spatio-temporal models for settings where space is viewed as continuous and time is taken as discrete.
  \end{itemize}
\end{itemize}

\subsection*{Gaussian Predictive Process Models for Large Spatial Data Sets \citep{Banerjee2008}} % ================================
\begin{itemize}
\item ``{\bf Spatial predictive process} - project the original (spatial) process onto a subspace that is generated by realizations of the original process at a specified set of locations.''
\item ``We regard the predictive process as a competing model specification with computational advantages, but induced by an underlying full rank process.''
\item Paraphrased: ``Our method similar to reduced rank kriging method proposed by Cressie and Johannesson (2008)... but theirs is ineffective for hierarchical models, that have random effects at second stage of specification, and no data to provide an empirical covariance function.'' (pg 829)
\item Knot selection: ``We need a criterion to decide between a regular grid and placing more knots where we have sampled more. One approach would be a so-called {\bf space filling knot selection} following the design ideas of Nychka and Saltzman (1998). Such designs are based on geometric criteria, measures of how well a given set of points covers the study region, independent of the assumed covariance function. Stevens and Olsen (2004) showed that spatial balance of design locations is more efficient than simple random sampling.''
\item ``A direct assessment of knot performance is comparison of the covariance function of the parent process with that of the predictive process, $\tilde{C}(\pmb{s}, \pmb{s}'; \pmb{\theta}) = \pmb{c}^{T}(\pmb{s};\pmb{\theta}) \cdot \pmb{C}^{*-1}(\pmb{\theta}) \cdot \pmb{c}(\pmb{s}';\pmb{\theta})$, where predictive process $\tilde{w}(\pmb{s}) \sim \text{GP}\{0, \tilde{C}(\cdot)\}$

\end{itemize}
\begin{enumerate}
\item $Y(\pmb{s}) = \pmb{x}^{T}(\pmb{s})\pmb{\beta} + w(\pmb{s}) + \epsilon(\pmb{s})$
    \begin{itemize}
    \item $\epsilon(\pmb{s}) \sim^{\text{{\it iid} }} N(0, \tau^{2})$
    \end{itemize}
\item $w(\pmb{s}) \sim GP\{0, C(\pmb{s}, \pmb{s}')\}$
      \begin{itemize}
      \item Gaussian process, covariance function $C(\pmb{s}, \pmb{s}')$
      \end{itemize}
\item $\pmb{Y} \sim N(\pmb{X\beta}, \Sigma_{\pmb{Y}})$
      \begin{itemize}
      \item $\Sigma_{\pmb{Y}} = C(\pmb{\theta}) + \tau^{2}\pmb{I}_{N}$
      \item $\pmb{X} = [\pmb{x}^{T}(\pmb{s}_{i})]_{i=1}^{n}$
      \item $C(\pmb{\theta}) = [C(\pmb{s}_{i}, \pmb{s}_{j}; \pmb{\theta})]_{i,j=1}^{n}$
      \end{itemize}

\item $\pmb{S}^{*} = \{\pmb{s}_{1}^{*}, \dots, \pmb{s}_{m}^{*}\}$ (knots)
\item $\pmb{w}^{*} = \left[w(\pmb{s}_{i}^{*})\right]_{i=1}^{m}$ (random effect at knots)
\item $\pmb{w}^{*} = \left[w(\pmb{s}_{i}^{*})\right]_{i=1}^{m} \sim \text{MVN}\{\pmb{0}, \pmb{C}^{*}(\pmb{\theta})\}$
            \begin{itemize}
            \item $\pmb{C}^{*}(\pmb{\theta}) = \left[C(\pmb{s}_{i}^{*}, \pmb{s}_{j}^{*})\right]_{i,j = 1}^{m}$ (Covariance of knots with themselves; $m \times m$)
            \end{itemize}
\item $\tilde{w}(\pmb{s}_{0}) = E[w(\pmb{s}_{0})|\pmb{w}^{*}]= \pmb{c}^{T}(\pmb{s}_{0};\pmb{\theta}) \cdot \pmb{C}^{*-1}(\pmb{\theta}) \cdot \pmb{w}^{*}$
            \begin{itemize}
            \item $\pmb{c}(\pmb{s}_{0};\pmb{\theta}) = \left[C(\pmb{s}_{0}, \pmb{s}_{j}^{*}; \pmb{\theta})\right]_{j = 1}^{m}$ (covariance of $\pmb{s}_{0}$ with knots; $m \times 1$ vector)
            \end{itemize}
\item $\tilde{w}(\pmb{s}) \sim \text{GP}\{0, \tilde{C}(\cdot)\}$
              \begin{itemize}
              \item $\tilde{w}(\pmb{s})$ is predictive process
              \item $\tilde{C}(\pmb{s}, \pmb{s}'; \pmb{\theta}) = \pmb{c}^{T}(\pmb{s};\pmb{\theta}) \cdot \pmb{C}^{*-1}(\pmb{\theta}) \cdot \pmb{c}(\pmb{s}';\pmb{\theta})$ ({\bf game changer})
              \item $\pmb{c}(\pmb{s};\pmb{\theta}) = \left[C(\pmb{s}, \pmb{s}_{j}^{*})\right]_{j = 1}^{m}$ (covariance of $\pmb{s}$ with knots)
              
              \end{itemize}
\item $Y(\pmb{s}) = \pmb{x}^{T}(\pmb{s})\pmb{\beta} + \tilde{w}(\pmb{s}) + \epsilon(\pmb{s})$
              \begin{itemize}
              \item {\bf Predictive process model}
              \item $\tilde{w}(\pmb{s})$ is spatially varying linear transformation of $\pmb{w}$.
              \end{itemize}
\end{enumerate}

\subsection*{spBayes for Large Point-Referenced Spatio-Temporal Data Models}
\begin{itemize}
\item See \verb|spBayes.R|
\item \verb|spGLM()| - ``fits univariate Bayesian generalized linear spatial regression models. Given a set of knots, spGLM will also fit a predictive process model'' (from help page) 
\item \begin{verbatim}
spGLM(formula, family="binomial", weights, 
      data = parent.frame(), coords, knots, 
      starting, tuning, priors, cov.model,
      amcmc, n.samples, verbose=TRUE,
      n.report=100, ...)
      \end{verbatim}
\item I don't understand: \verb|tuning| and \verb|amcmc| (adaptive MCMC) 
\item \verb|tuning| - a list with each tag corresponding to a parameter name. {\bf The value portion of each tag defines the variance of the Metropolis sampler Normal proposal distribution.} 
\item Exponential Covariance: From \verb|spBayes| spatial binomial example. (from \verb|spBayes()| cran documentation)
      \begin{itemize}
      \item \verb|R <- sigma.sq*exp(-phi*as.matrix(dist(coords)))|
      \item $\rightarrow Cov(s_{1}, s_{2}) = \sigma^{2}exp[-\phi \cdot d(s_{1},s_{2})]$
      \end{itemize}
\item \verb|priors| argument - a list with each tag corresponding to a parameter name. Valid tags are \verb|sigma.sq.ig|, \verb|phi.unif|, \verb|nu.unif|, \verb|beta.norm|, and \verb|beta.flat|. 
        \begin{itemize}
        \item Variance parameter \verb|simga.sq| is assumed to follow an inverse-Gamma distribution, 
        \item whereas the spatial decay \verb|phi| and smoothness \verb|nu| parameters are assumed to follow Uniform distributions. 
        \item The hyperparameters of the inverse-Gamma are passed as a vector of length two, with the first and second elements corresponding to the shape and scale, respectively. 
        \item The hyperparameters of the Uniform are also passed as a vector of length two with the first and second elements corresponding to the lower and upper support, respectively. 
        \item If the regression coefficients are each assumed to follow a Normal distribution, i.e., \verb|beta.norm|, then mean and variance hyperparameters are passed as the first and second list elements, respectively. 
        \item If beta is assumed flat then no arguments are passed. The default is a flat prior.
        \end{itemize}
\end{itemize}

\subsection*{Fitting predictive process model with spBayes spGLM() - To do's} % ==========================================
\begin{itemize}
\item Confirm it is running the model I think it is (Use Stan to confirm?)
\item Need to run diagnostics after (working on it)
\item Understand adaptive MCMC (punt)
\item Understand tuning (variance of proposal distribution)
\item Choose knots \citep{Nychka1998} (can wait)
\item Posterior credible intervals (??) (Gelman text)
\end{itemize}

\subsection*{Charlotte Meeting} % ===================================
\begin{itemize}
\item AMCMC - later
\item Choose knots - later
\item {\bf Understand tuning - now}
\item Diagnostics - after
\item Run back to stan carrying predictive process models? Not just yet.
\end{itemize}

* {\bf Note: grab Gelman's Bayesian book from office, read up on Metropolis-Hastings algorithm} \\

* Metropolis Note: when drawing from the proposal distribution, $Q(\cdot|\theta_{t})$, if $Q$ is the Normal, then $Q(\cdot|\theta_{t})$ means $N(\theta_{t}, \cdot)$... as I suspected---but is sloppy notation.

\section*{Week of January 30, 2017}

\subsection*{spBayes} 
\begin{itemize}
\item Current problem: the M-H MCMC acceptance rates are way too low.
\item Problem question: Is \verb|spGLM()| using proposal distributions with different support than parameters. e.g. Normal for $\sigma^{2}$?
\item Right now trying to better understand M-H algorithm, using {\bf BanerjeeLecture.pdf}, among other resources. Non-acceptance means r is very small, because perhaps the proposal variance is too big, and the jumps away are drastically reducing the likelihood.
\item Trying {\bf adaptive MCMC again, and it works}. It's slower, and, at n = 100 pitches, not converging. At n = 200 pitches, it seeeeems {\it closer} to converging (non-technical assessment!). Hijacked adaptive parameters:
\begin{verbatim}
	parameter	acceptance	tuning
	beta[0]		44.0		0.56789
	beta[1]		40.0		0.19771
	beta[2]		38.0		1.12920
	beta[3]		40.0		0.05303
	beta[4]		60.0		0.80597
	beta[5]		64.0		0.26464
	beta[6]		82.0		0.04910
	sigma.sq	52.0		0.25585
	phi		    70.0		0.13728
\end{verbatim}
\item I think the thing to do now is add some knots at my variable resolution grid box centers, and see how it does!
\item Variable Res. heat map knots
      \begin{itemize}
      \item Using 97 knots resulting from an $n_{b} < 200$ cutoff, and n = 300 observations, \verb|spGLM()| took about 3 mins. The trace plots did not suggest convergence.
      \item Same, but n = 500, about 4 mins. Still does not look convergent. (10,000 iterations)
      \item n = 1000, 6.7 mins. Convergence *maybe* a smidge better. But time is good!! (10,000 iterations)
      \item n=1000; 30,000 iterations; looks better. Not exactly convergent, but closer.
      \item 7 mins, n = 1000, knots = 49, 30K samples
      \item X mins, n = 9172, knots = 49, ``80,000 samples completed'' but the dreaded rainbow pinwheel never went away when computer tried to resuscitate in morning. We'll never know...
      \item 54 mins, n = 3000, kn = 49, 80K samples   
      \begin{verbatim}
               Mean     SD  Naive SE Time-series SE
(Intercept) -4.2154 1.2638 0.0044682       0.240307
r            1.6878 0.9192 0.0032499       0.270467
theta       -0.2776 2.0000 0.0070711       0.275294
r2          -0.4029 0.1878 0.0006639       0.037039
theta2      -2.5969 1.3842 0.0048940       0.097303
r_theta     -1.7129 1.1326 0.0040042       0.202156
r2_theta2   -0.5211 0.2922 0.0010331       0.030658
sigma.sq     0.3920 0.2736 0.0009674       0.009469
phi          0.4095 0.2872 0.0010156       0.018573
\end{verbatim}
      \end{itemize}
\end{itemize}
\includegraphics[scale=.1]{n3000.jpeg} 
\begin{itemize}
      

\item Need to think. Digest. Good progress.
\item Recall: Stan guys suggested identifiability and convergence issues can result from... something. Should look back at what they said. 
\item Stan implementation online uses version from ``Improved...'' follow-up paper Finley wrote.
\end{itemize}

\subsection*{Shiny it Up!}
\begin{itemize}
\item Two heat maps to Shiny: 
  \begin{enumerate}
  \item Empirical Var Res; Normal approx. to Binomial: $\hat{p} \pm z_{1-\alpha/2}*\sqrt{\hat{p}(1-\hat{p})/n}$
  \item Polar model (w/o random effect); each point has its own SE; MLE estimates are Normally distributed (gotta double check in GLM text); use \verb|predict()| function; use \verb|Field_of_Dreams3.R| as a reference; SE of the fit is calculated, so only new $z_{1-\alpha/2}$ will need to be plugged into fit $\pm z_{1-\alpha/2}*$se.fit each time
  \end{enumerate}
\item Not gonna be too bad.
\item Confusing the heck out of me. Review.
  \begin{itemize}
  \item $\alpha$ = P(No coverage)
  \item CI\% = 1 - $\alpha$
  \item $P(Z > z_{\alpha}) = \alpha$ 
  \begin{verbatim}
  > qnorm(0.05, lower.tail = FALSE)
  [1] 1.644854
  \end{verbatim}
  So $P(Z > 1.645) = 0.05$. Think of Normal curve. 
  \item For 95\% CI, we say $\alpha = 0.05$.
        \begin{itemize}
        \item This necessitates $z_{1-\alpha/2}$ for CI.
        \item $\hat{p} \pm z_{1-\alpha/2}*\sqrt{\hat{p}(1-\hat{p})/n}$
        \end{itemize}


  \end{itemize}
\item I got... my Shiny app... TO WORK!!! 
\end{itemize}

\subsection*{Meeting}

\subsubsection*{Items to report}
\begin{itemize}
\item Mango interview, R Coding Test
  \begin{itemize}
  \item Alix: progress updates! Tell them if anything happens. 
  \item Definitely give them a heads up if they are going to get a call (as a reference).
  \item Alix said: ``You're one of the most curious people I've ever met. It's a great part of who you are. Always reading all those books, walking around with... [imitates fiddlink] in your hand.'' She said I could/should have added that to my well-rounded answer.
  \item Charlotte/Alix said the same job here (statistical consultant) would bring in \$120,000 - \$140,000
  \end{itemize}
\item Heat Map CI Shiny app
  \begin{itemize}
  \item Add simple, point and click CI??? (Charlotte's challenge)
  \end{itemize}
\item spBayes works-ish, trace plots
  \begin{itemize}
  \item Trace plots indicate non-convergence, but this should not discourage, but instead encourage more iterations. More more more iterations. 
  \item Also keep in mind we're reducing information in some way, using knots only.
  \item Both thought 97 was a lot.
  \item Alix very excited it's working. ``Andy is going to be so excited; we're using his package on baseball data!''
  \end{itemize}
\end{itemize}

\subsection*{Evaluate Inverse you say?? Yes.}
\begin{itemize}
\item logit\{EY(s)\} = $\pmb{X}(s)\pmb{\beta} + Z(s)$, with $Z(s) \sim MVN\{\pmb{0}, \Sigma_{s}\}$
\item $f(\pmb{\beta}, \phi, \sigma^{2}, \pmb{Z}|\pmb{Y}) \propto f(\pmb{Y}|\pmb{\beta}, \phi, \sigma^{2}, \pmb{Z})f(\pmb{\beta})f(\pmb{Z}|\phi, \sigma^{2})f(\phi)f(\sigma^{2})$
\item M-H proposal, iteration i: $Z_{10,i}$
$$ r = \frac{ f(Z_{10,i}|\pmb{Z}_{1:9,i},\pmb{Z}_{11:n,i-1}, \pmb{\beta}_{i-1}, \phi_{i}, \sigma^{2}_{i})}{f(Z_{10,i-1}|\pmb{Z}_{1:9,i},\pmb{Z}_{11:n,i-1}, \pmb{\beta}_{i-1}, \phi_{i}, \sigma^{2}_{i})} $$
 
\item Note: $f(z_{1}, z_{2}, z_{3}|\pmb{Y}) = f(z_{1}|z_{2},z_{3},\pmb{Y})f(z_{2},z_{3}|\pmb{Y})$. So... $$r \propto \frac{f(\pmb{Y}|\pmb{\theta}_{i})f(\pmb{\beta})f(Z_{10,i}|\pmb{Z}_{1:9,i},\pmb{Z}_{11:n,i-1}, \phi_{i}, \sigma^{2}_{i})f(\pmb{Z}_{1:9,i},\pmb{Z}_{11:n,i-1}|\phi_{i}, \sigma^{2}_{i})f(\phi)f(\sigma^{2})} {f(\pmb{Y}|\pmb{\theta}_{i-1})f(\pmb{\beta})f(Z_{10,i-1}|\pmb{Z}_{1:9,i},\pmb{Z}_{11:n,i-1}, \phi_{i}, \sigma^{2}_{i})f(\pmb{Z}_{1:9,i},\pmb{Z}_{11:n,i-1}|\phi_{i}, \sigma^{2}_{i})f(\phi)f(\sigma^{2})}$$

$$ r \propto \frac{f(\pmb{Y}|\pmb{\theta}_{i})f(Z_{10,i}|\pmb{Z}_{1:9,i},\pmb{Z}_{11:n,i-1}, \phi_{i}, \sigma^{2}_{i})}
{f(\pmb{Y}|\pmb{\theta}_{i-1})f(Z_{10,i-1}|\pmb{Z}_{1:9,i},\pmb{Z}_{11:n,i-1}, \phi_{i}, \sigma^{2}_{i})} $$

\item And $f(\pmb{Z})$, $f(Z_{i}|\pmb{Z}_{-i})$, etc. are $MVN\{\cdot,\Sigma^{*}\}$, where $\Sigma^{*}$ either is, or is some function of, $\Sigma_{\pmb{s}}$; with PDF kernal containing $\Sigma_{\pmb{s}}^{*-1}$, (containing $\phi_{i}, \sigma^{2}_{i})$.

\end{itemize}

\subsection*{Next Steps}
\begin{itemize}
\item Convergence, more iterations for \verb|m1 <- spGLM(...)|
\item Add Charlotte requested feature to Shiny app
\item Add var. res. map to shiny app
\item Write. Add ending to chapter one. Start chapter two: introduce random effect, SPGLMM, stan/dimensionality issues/refinement (appendix?), Finley predictive process model dimension reduction
\item Fit predictive process model in Stan?
\end{itemize}

\section*{February 6, 2017}

\begin{itemize}
\item Fit predictive process model in Stan. 
        \begin{itemize}
        \item \citep{Neal2011} Figure 5.4, Figure 5.5, Figure 5.6 illustrate the much better, {\it smarter} mixing with HMC over random-walk Metropolis.
        \item Samples more correlated from random-walk Metropolis \citep{Neal2011}
        \item Speed (LondonR rstan presentation)
        \item No-U-Turn sampler \citep{Hoffman2014}; 
        \item rstan: ``full Bayesian inference using the No-U-Turn sampler (NUTS), a variant of Hamiltonian Monte
Carlo (HMC)'' \citep{rSTANtheMan}
        \end{itemize}
\item Idea of day: Collapse all locations to knot locations, estimate $\phi$ and $\sigma^{2}$. What else?
\item Going to try to create $N \times M$ covariance(obs, knots) matrix in stan; pg 387 in Stan the Manual; did it. 
\item Random side-note - {\bf Clayton Copulas} look like my hit distribution.
\item Maybe we should do a WAY BETTER JOB of what Cross and Sylvan did. Use league averages, exponential covariance structure with predictive process model knots to estimate covariance parameters. Get rid of hokey r, $\theta$ business.
\item Could have Ben show me how to access the mysterious, high power processors we have access to.
\end{itemize}

\subsection*{Knot Selection} % =====================================
\citep{Banerjee2008}
\begin{itemize}
\item In forest biomass example \cite{Banerjee2008} says ``With only 36 knots the distance between adjacent knots (40 km) seemed to exceed the effective spatial ranges that were supported by the data and {\bf led to unreliable convergence of process parameters}.'' (Boldface mine)
\item ``...knot selection is required and as we demonstrated in Section 5.1 some sensitivity to the number of knots is expected. Although for most applications a reasonable grid of knots should lead to robust inference, with {\bf fewer knots} the separation between them increases and estimating random fields with {\bf fine scale spatial} dependence becomes {\bf difficult}. Indeed, learning about {\bf fine scale spatial dependence} is always a {\bf challenge} (see, for example, Cressie (1993), page 114).'' (boldface mine)
\end{itemize}

\subsection*{Meeting}
Charlotte
\begin{itemize}
\item Get around ``for'' loop somehow?
\item Try binning distances idea?
\item Look at HMC convergence more closely, make sure this is worth it.
\item Websites for HTML/CSS (Codeacademy), \\ C++ (http://adv-r.had.co.nz/Rcpp.html), \\ GitHub (http://happygitwithr.com/)
\end{itemize}
Alix
\begin{itemize}
\item Email Debashis again if I feel like it, drop Alix's name
\item {\bf Anything in paper about how to choose the number of knots? Look into it.}
\item Just choose a hitter with fewer swings. Russian pencil method.
\item Try with way fewer knots. Ten knots.
\item Email Andy if I feel like it.
\item Alix doesn't want to try the aggregating thing yet, says I haven't struggled with predictive process models enough yet. (Admittedly, it has only been a few weeks or so)
\item Fix $\beta$s, estimate covariance parameters? Try it.
\item Start thinking about cross-validation.
\item Run the long, five hour estimations, do something else while it goes.
\item Alix thinks--and I reluctantly agree--that sometimes I am too quick to say "good enough." ...as I was with package question on the Mango ``R Coding Test.''
\end{itemize}

\subsection*{Fix Covariates}
\begin{itemize}
\item spBayes, n=3000, one chain, 37500 iteration
\begin{verbatim}
           Mean     SD  Naive SE Time-series SE
XB       1.0000 0.0000 0.0000000       0.000000
sigma.sq 0.2921 0.1850 0.0009551       0.006772
phi      0.4318 0.2886 0.0014905       0.032011
\end{verbatim}
\includegraphics[scale=.05]{Images/n3000_XB.jpeg} \\

\end{itemize}

\section*{February 13, 2017}

\subsection*{Miscellaneous}
\begin{itemize}
\item ``Approximate Bayesian Inference for Latent Gaussian models using integrated nested Laplace approximations'' \citep{Rue2009} This looks cool. I want to try this.
\item Review ``posterior predictive distribution'' (need Bayesian book)
\item Scoring rules, \citep{Bickel2007}
\item Diagnose {\bf convergence}
\item ``As with any knot based method, selection of knots is a challenging problem... Suppose for the moment that $m$ is given. We are essentially dealing with a problem that is analogous to a {\bf spatial design problem}...'' \citep{Finley2009}
\item {\bf Spatial design problem}
\item ``There is a rich literature in spatial design with is summarized in, e.g., the recent paper of \cite{Xia2006}''
\item ``Approximately optimal spatial design approaches for environmental health data.'' \citep{Xia2006}
\item ``Spatial sampling design for parameter estimation of the covariance function'' \citep{Zhu2005}
\item {\bf KL distance} - ``Expressed in the language of Bayesian inference, the Kullback–Leibler divergence from Q to P, denoted $D_{KL}(P||Q)$, is a measure of the information gained when one revises one's beliefs from the prior probability distribution Q to the posterior probability distribution P. In other words, it is the amount of information lost when Q is used to approximate P.'' (Wikipedia)
\end{itemize}

% ================= ================ ================= =============== =========
\subsection*{``Hierarchical spatial models for predicting tree species assemblages across large domains''\citep{Finley2009_2}; multinomial}
\begin{itemize}
\item ``While most of the models we formulate can possibly be estimated using maximum likelihood or variants thereof, we adopt a Bayesian approach [e.g., Gelfand et al. (2003)]. This is attractive, as it offers exact inference for the random spatial coefficients, and that too with non-Gaussian data, by delivering an entire posterior distribution at both observed and unobserved locations. Spatial interpolation for processes that are neither observed nor arise as residuals appears inaccessible with classical likelihood-based methods. On the other hand, Bayesian model fitting involves rather specialized Markov chain Monte Carlo (MCMC) methods [see, e.g., Robert and Casella (2005)] that raise concerns about computational expense and reproducibility of inference. These concerns have, however, started to wane with the delivery of relatively simpler R packages (www.r-project.org), including mcmc, MCMCpack, geoRglm and spBayes, that help automate such methods and diagnose convergence.''
\item ``While our {\bf primary contribution here lies in the novel application}, we also offer several methodological advancements.''
\item ``Three MCMC chains of 75,000 iterations were run for each model. Then posterior inference was based on 3 $\times$ 50,000 = 150,000 post burn-in samples.''
\item Justifications (pg 7) for PPMs over other methods, for dealing with large spatial datasets: (i) adapts easily to multivariate processes, (ii) spatial interpolation convenient, (iii) non-Gaussian, (iv) Flexible, multi-parameter with several hyperparameters (over INLA, Integrated Nested Laplace Approximations)
\item ``With irregular locations, however, we may encounter substantial areas of sparse observations where placing would amount to “wastage,” possibly leading to inflated variance estimates and slower convergence. More practical space-covering designs [e.g., Royle and Nychka (1998)] can yield a representative collection of knots that better cover the domain.''
    \begin{itemize}
    \item This could be a contribution. Compare my ``var-res-grid centers as knots'' to Nychka.
    \end{itemize}
\item ``We consider several different scoring rules to evaluate the predictive performance of the candidate models. A scoring rule provides a summary measure for evaluating a probabilistic prediction given the predictive distribution and the observed outcome. '' pg 10
\item ``\cite{Gneiting2007} offer four scoring rules for prediction of categorical variables'' page 10. 
  \begin{itemize}
  \item These guys have other papers, and LOTS of citations.
  \end{itemize}
\item {\bf Confusion matrix} - (google search) A confusion matrix is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known. (google search)
\item ``We also compare our spatially-varying multinomial logistic regression models to common benchmark methods'' page 11
\item ``For estimating predicive process models, we used 154, 200, and 254 knots over the domain.''
  \begin{itemize}
  \item Could the efficacy of the PPM with X knots inform, in reverse, a method for choosing the cutoff?
  \end{itemize}
\item ``The spatially-varying coefficients model was the most computationally challenging, with each chain of the 254 knot model taking ~5 hours to complete. {\bf The \verb|CODA| package in R (www.r-project.org) was used to diagnose convergence by monitoring mixing using Gelman–Rubin diagnostics and autocorrelations} [see, e.g., Gelman et al. (2004), Section 11.6]. Acceptable convergence was diagnosed within 25,000 iterations and, therefore, 150,000 samples (3 x 50,000) were retained for posterior analysis.''
  \begin{itemize}
  \item DIAGNOSTICS. 
  \end{itemize}
\end{itemize}

\subsection*{Dissertation (Middle Chunks) Outline} % ============= ============ ========
\begin{itemize}
\item Var-Res heat maps
  \begin{itemize}
  \item Heat maps
  \item Shiny
  \end{itemize}
\item MLE model
  \begin{itemize}
  \item Profile plots to show interpretability potential
  \end{itemize}
\item Random effect model
  \begin{itemize}
  \item MCMC - Markov Chain Monte Carlo
  \item Stan - HMC
          \begin{itemize}
          \item Code code code (communications, and Trangucci2017 handout)
          \item Hamiltonian Monte Carlo \citep{Neal2011}
          \end{itemize}
  \item spBayes, Predictive process models
          \begin{itemize}
          \item Knot quantity - seems like comparison at range of values
          \item placement decisions - my var-res-grid centers vs. \citep{Nychka1998}
          \item Metropolis nuts and bolts
          \item {\bf Convergence} - \cite{Finley2011} talks about convergence, Gelman-Rubin diagnostics (\citep{Gelman2014}), mixing, and \verb|CODA| package in R, classification confusion matrix, on page 12.
          \end{itemize}
  \end{itemize}
\item Include Cross kriging, empirical Bayes boxes
\item Model validation, comparison
      \begin{itemize}
      \item Hosmer-Lemeshow - goodness of fit test
      \item \cite{Finley2011} has ``5.1 Model validation and benchmark comparisons'' section
      \item \cite{Finley2011} cites \cite{Gneiting2007} regarding scoring rules, etc., as have 1577 other papers
      \end{itemize}
\end{itemize}

\subsection*{Scoring rule}
\begin{itemize}
\item Wikipedia: ``In decision theory, a score function, or scoring rule, measures the accuracy of probabilistic predictions. It is applicable to tasks in which predictions must assign probabilities to a set of mutually exclusive discrete outcomes. The set of possible outcomes can be either binary or categorical in nature, and the probabilities assigned to this set of outcomes must sum to one (where each individual probability is in the range of 0 to 1). 

...Proper scoring rules are used in meteorology, finance, and pattern classification where a forecaster or algorithm will attempt to minimize the average score to yield refined, calibrated probabilities (i.e. accurate probabilities).''
\item Wikipedia: ``An example of probabilistic forecasting is in meteorology where a weather forecaster may give the probability of rain on the next day. One could note the number of times that a 25\% probability was quoted, over a long period, and compare this with the actual proportion of times that rain fell. If the actual percentage was substantially different from the stated probability we say that the forecaster is poorly calibrated.''
\item Examples: logarithmic, Brier/quadratic, spherical
\end{itemize}

\subsection*{Strictly proper scoring rules, prediction, and estimation }
\citep{Gneiting2007}
\begin{itemize}
\item 
\end{itemize}



\subsection*{An Algorithm for the Construction of Spatial Coverage Designs... \citep{Nychka1998}}
\begin{itemize}
\item ``Space-filling ``coverage'' designs are spatial sampling plans which optimize a distance-based criterion''
\end{itemize}

\subsection*{Spatial Sampling Design}
\begin{itemize}
\item Note: Random effect is Gaussian, response is not.
\item One option: simulation study
  \begin{enumerate}
  \item Define multiple knot structures $k = 1,...,n_{k}$
  \item Generate GRF data, using exponential covariance
  \item Estimate parameters using each knot structure (Bayesian PPM approach; same code, $X\beta$ fixed), store estimates; 
  \item Iterate
  \item Compare (via variance?) results
  \end{enumerate}
\item ``2.4 Selection of knots'' \citep{Banerjee2008} is a treasure trove of talk about this.
\item ``Bayesian Sampling Design'' \citep{Diggle2006} looks pretty applicable. (i) lattice plus close pairs, and (ii) lattice plus infill
\end{itemize}



\subsection*{Gaussian Predictive Process Models for Large Spatial Data Sets}
\citep{Banerjee2008}
\begin{itemize}
\item ``We project the original process onto a subspace that is generated by realizations of the original process at a specified set of locations.''
\item ``The w(s) are spatial random effects, providing local adjustment (with structured dependence) to the mean, interpreted as capturing the effect of unmeasured or unobserved covariates with spatial pattern.''
\item ``We regard the predictive process model as a competing model with computational advantages.''
\item Other approaches ``may be challenging for the hierarchical models'' here, with ``spatial modelling with random effects at the second stage of the specification. We have no 'data' to provide an empirical covariance function.''
\item ``A direct assessment of knot performance is comparison of the covariance function of the parent process with that of the predictive process...''
\item ``There is little additional information in varying $m$ for a given $\phi$ and $\nu$ (Matern params). What matters is the size of the range relative to the spacing of the grid for the knots.''
\item ``Choice of $m$ is governed by computational cost and sensitivity to choice... implement the analysis over different choices of $m$... consider run time... stability of predictive inference.''
\item ``...improvement in estimation with increasing number of knots.''
\item ``Tables 1--3 suggest that estimation is more sensitive to the number of knots than to the underlying design.''
\item Convergence diagnostics, CODA package
\item ``With only 36 knots the distance between adjacent knots (40 km) seemed to exceed the effective spatial ranges that were supported by the data and led to unreliable convergence of process parameters.''
\item ``Our examples in Section 5.1 showed that even with fairly complex underlying spatial structures the predictive process model could effectively capture most of the spatial parameters with 529 knots (irrespectively of whether the total number of locations was 3000 or 15000).
\item ``In fact, the predictive process approach within a full MCMC implementation is perhaps limited to the order of $10^{4}$ observations on modest single-processor machines.''
\end{itemize}

\section*{20 February 2017}

\section*{****************** Begin INLA-SPDE-RINLA ***************}

\subsection*{Debashis meeting}
\begin{itemize}
\item INLA is oversold; Does Matern $\nu = 1, 2, 3$ only (no exponential, $\nu = 1/2$)
\item Matern $\nu = 1$ is reasonable-ish.
\item Could add structure to cov. matrix, make it sparse, make cov = 0 for $d > d_{0}$
\item SPDE is oversell too.
\item INLA is basically Taylor series
\item Will be bias in estimates, for binary data in particular.
\item MCMC might work better with sparse cov matrix, R has a package for sparse matrices; does Stan use sparse matrices?
\end{itemize}

\section*{February 27, 2017}

\subsubsection*{Matern Covariance (1)}
$$ C_{\nu}(d) = \sigma^{2} \frac{2^{1 - \nu}}{\Gamma(\nu)} \left( \sqrt{2 \nu} \frac{d}{\rho} \right)^{\nu} K_{\nu} \left( \sqrt{2\nu}\frac{d}{\rho} \right) $$
Where $\Gamma$ is the gamma function, and $K_{\nu}$ is the modified Bessel function, $\rho$ and $\nu$ are non-negative covariance parameters
\begin{itemize}
\item Recall, $\Gamma(n) = (n-1)!$
\item $\nu = 1/2$ gives exponential covariance 
$$C_{1/2}(d) = \sigma^{2} exp(-d/\rho)$$
\item Stationary and isotropic if Euclidean distance
\end{itemize}


\subsection*{Miscellaneous}
\begin{itemize}
\item Covariance tapering? $d < d_{0} \rightarrow d = 0$, then what?
\item {\bf range parameter: the Euclidean distance where $x(s_{0})$ and $x(s_{1})$ are almost independent.} \citep{Lindgren2011}
\end{itemize}

\section*{Meeting}
\begin{itemize}
\item I {\bf CAN} make {\bf half} of my talk ``Lessons Learned''!!
\item ``Lessons Learned'' 
  \begin{itemize}
  \item Zero-th problem (Journal club)
  \item JSM talk, already published -- do a lit review!
  \item Write and show
  \item ``Secrets of Research Success'' -Hugh Kearns
  \item Imposter syndrome
  \item Compare yourself to others $\rightarrow$ ruin!
  \item Koutsoyiannis
  \item Richard Feynman
  \item ``Thinking, Fast and Slow''
  \item ``Set a pace you can keep.''
  \end{itemize}
\item INLA
\item ``You're ready to be done.'' -Alix, as in ``You have gained the research skills you need to go out there and be successful.''
\item ``This is great! You're coming in here telling {\bf  me} what your dissertation is!''
\item My plan for chapters is legit.
\item Talked about family, job search, research, seminar talk, Chris Wolf.
\end{itemize}

\section*{March 6, 2017}

\subsection*{An explicit link between Gaussian fields and Gaussian Markov random fields: the stochastic partial differential equation approach \citep{Lindgren2011}}
\begin{itemize}
\item ``A GMRF is a discretely indexed Gaussian field x, where the full conditionals'' depend only on a small set of symmetric neighbor relationships, which yields sparse matrices that lend themselves to approximations---INLA
\item The GMRF ``computational gain comes from the fact that the zero pattern of the precision matrix Q (the inverse covariance matrix) relates directly to the notion of neighbours...''
\item Matrix {\bf Q: the inverse covariance matrix}
\item ``The result is a basis function representation with piecewise linear basis functions, and Gaussian weights with Markov dependencies determined by a general triangulation of the domain.'' 
  \begin{itemize}
  \item Recall ``basis'' from linear algebra, where a set of linearly independent vectors span a space
  \item function space - space of {\it functions}
  \item basis functions - a set of functions from which can build any function in the function space
  \end{itemize}

\item Matern (2):
$$r(\pmb{u}, \pmb{v}) = \frac{\sigma^{2}}{2^{\nu - 1}\Gamma(\nu)}(\kappa||\pmb{u} - \pmb{v}||)^{\nu}K_{\nu}(\kappa||\pmb{u} - \pmb{v}||)$$
      \begin{itemize}
      \item scaling parameter $\kappa = \rho$ range parameter
      \item Empirically derived: $\rho \equiv \sqrt{8\nu}/\kappa$
      \end{itemize}
\item (identity) Linear fractional SPDE:
$$ (\kappa^{2} - \Delta)^{\alpha/2} x(\pmb{u}) = \mathcal{W}(\pmb{u}) $$
      \begin{itemize}
      \item $\alpha = \nu + d/2$ 
      \item Note: d=2 for $\mathbb{R}^{2}$
      \end{itemize}
\item $\Delta$ is Laplacian:
$$ \Delta = \sum_{i=1}^{d} \frac{\partial^{2}}{\partial x_{i}^{2}} $$
\item Marginal Variance:
$$\sigma^{2} = \frac{\Gamma(\nu)}{\Gamma(\nu + d/2)(4\pi)^{d/2}\kappa^{2\nu}}$$
\end{itemize}

\subsection*{Bayesian Spatial Modelling with R-INLA \citep{Lindgren2015}}
\begin{itemize}
\item ``...as discussed in Lindgren et al. (2011), one can express a large class of random field models as solutions to continuous domain stochastic partial differential equations (SPDEs), and write down explicit links between the parameters of each SPDE and the elements of precision matrices for weights in a discrete basis function representation.''
\item ``{\bf An alternative to traditional covariance based modelling is to use SPDEs, but carry out the practical computations using Gaussian Markov random field (GMRF) representations. This is done by approximating the full set of spatial random functions with weighted sums of simple basis functions, which allows us to hold on to the continuous interpretation of space, while the computational algorithms only see discrete structures with Markov properties.} Beyond the main paper Lindgren et al. (2011), this is further discussed by Simpson, Lindgren, and Rue (2012a,b).'' \citep{Simpson2012}, \citep{Simpson2012b}
\item \cite{Simpson2012} give a {\bf fantastic picture} for conveying the action of SPDE method
\item ``The simplest model for (spatial field) $x(\pmb{s})$ currently implemented in R-INLA is the SPDE/GMRF version of the stationary Matern family, obtained as the stationary solutions to
$$ (\kappa^{2} - \Delta)^{\alpha/2}(\tau x(\pmb{s})) = \mathcal{W}(\pmb{s})\text{, } \pmb{s} \in \Omega $$ where
    \begin{itemize}
    \item $\Delta$ is the Laplacian
    \item $\kappa$ is the spatial scale parameter
    \item $\alpha$ controls the smoothness of the realisations
    \item $\tau$ controls the variance
    \item $\Omega$ is the spatial domain
    \item $\mathcal{W}(\pmb{s})$ is a Gaussian spatial white noise process
    \end{itemize}
Whittle (1954, 1963) shows stationary solutions on $\mathbb{R}^{d}$ have Matern covariances,
$$\text{COV}(x(\pmb{0}), x(\pmb{s})) = \frac{\sigma^{2}}{2^{\nu - 1}\Gamma(\nu)}(\kappa||s||)^{\nu}K_{\nu}(\kappa||s||)$$
The parameters in the two formulations are coupled so that the Matern smoothness is $\nu = \alpha - d/2$ and marginal variance is
$$\sigma^{2} = \frac{\Gamma(\nu)}{\Gamma(\alpha)(4\pi)^{d/2}\kappa^{2\nu}\tau^{2}}$$
Exponential covariance: $\nu = 1/2$; (i) for $d = 1 \rightarrow  \alpha = 1$, (ii) for $d = 2 \rightarrow \alpha = 3/2$''
\item ``The models discussed in \cite{Lindgren2011} and implemented in R-INLA are built on a basis representation
$$ x(\pmb{s}) = \sum_{k=1}^{n} \psi_{k}(\pmb{s})x_{k}$$
where
      \begin{itemize}
      \item $\psi_{k}(\cdot)$ are deterministic basis functions, and
      \item the joint distribution of the weight vector $\pmb{x} = \{x_{1},\dots,x_{n}\}$ is chosen so that the distribution of the functions $x(\pmb{s})$ approximates the distribution of solutions to the SPDE on the domain.''
      \item piecewise polynomial basis functions
      \item use {\bf Finite Element Method - project the SPDE onto the basis representation} ...which is GMRF. \citep{Simpson2012}
      \end{itemize}
\item Bayesian Inference (REALLY FREAKIN' GOOD SUMMARY)
        \begin{enumerate}
        \item $\tilde{p}(\pmb{x}|\pmb{\theta, y})$ by Gaussian approximation
        \item Posterior mode: $\pmb{x}^{*}(\pmb{\theta}) = \text{argmax}_{x}p(\pmb{x}|\pmb{\theta, y})$
        \item Laplace approximation
        $$ p(\pmb{\theta} | \pmb{y}) \propto \frac{p(\pmb{\theta, y, x})}{p(\pmb{x}|\pmb{\theta, y})} \Big|_{\pmb{x} = \pmb{x}^{*}(\pmb{\theta})} \approx \frac{p(\pmb{\theta, y, x})}{\tilde{p}(\pmb{x}|\pmb{\theta, y})} \Big|_{\pmb{x} = \pmb{x}^{*}(\pmb{\theta})}$$
        Approximate (unnormalized) posterior density for $\pmb{\theta}$ at any point, and numerical optimization to find mode of posterior.
        \item Numerical integration:
        $$ p(\theta_{k} | \pmb{y}) \approx \int \tilde{p}(\pmb{\theta}|\pmb{y}) d\pmb{\theta}_{-k} $$
        \item Numerical integration:
        $$ p(x_{j} | \pmb{y}) \approx \int \tilde{p}(x_{j}|\pmb{\theta, y})\tilde{p}(\pmb{\theta}|\pmb{y}) d\pmb{\theta} $$
        \end{enumerate}
\end{itemize}

\subsection*{Miscellaneous}
I got \verb|inla(...)| to run!!
\begin{itemize}
\item \verb|geoR| package
\item I am trying to generate Matern data with \verb|grf(...)|, then estimate parameters with \verb|inla(...)|. However, basic input to \verb|inla(...)| implies INLA, without SPDE connection, and thus data on a grid. This means using my actual pitch locations from \verb|hitter| will not work. \\
\item Try to generate data on a grid, use \verb|inla(...)| to estimate parameters
\item inverse Precision matrix equals covariance matrix: $Q^{-1} = \Sigma$
\item Precision matrix Q equals inverse covariance matrix: $Q = \Sigma^{-1}$
\end{itemize}

\subsection*{Simulate Matern data}
\verb|geoR| package, \verb|grf(...)| function
\subsection*{Matern (3)}
$$ \rho(u; \phi, \kappa) = \{2^{\kappa-1}\Gamma(\kappa)\}^{-1}(u/\phi)^{\kappa}K_{\kappa}(u/\phi)$$
  \begin{itemize}
  \item $u$ - vector/matrix/array with distances between pairs of data locations
  \item $\phi$ - range parameter, $>0$
  \item $\kappa$ - smoothness parameter, $>0$
  \item $K_{\kappa}(\cdot)$ - modified Bessel function of third kind of order $\kappa$
  \end{itemize}
  
\subsection*{Matern (4)}
\citep{Schabenberger2004}
$$ C(h) = \sigma^{2}\frac{1}{\Gamma(\nu)}\left(\frac{\theta h}{2}\right)^{\nu}2K_{\nu}(\theta h) \text{,   } \nu >0, \theta > 0 $$
\begin{itemize}
\item $\theta$ governs range of spatial dependence
\item $\nu$: smoothness increases with $\nu$
\item Page 143: The Matern Class of Covariance Functions
\item Page 199: ${\bf THREE}$ Matern parameterizations
\item Page 210: Bessel Functions
\item Whittle Model, $\nu = 1$ (INLA/SPDE world, I think)
$$ C(h) = \sigma^{2}\theta h K_{1}(\theta h) $$
Whittle considered this the ``elementary model'' in $\mathbb{R}^{2}$
\end{itemize}

\subsection*{Stochastic Calculus}

Stochastic Differential Equation (wikipedia)
\begin{itemize}
\item ``A heuristic but helpful interpretation of the stochastic differential equation (of continuous time stochastic process $X_{t}$) is that in a small time interval of length $\delta$ the stochastic process $X_{t}$ changes its value by an amount that is normally distributed (for example) with expectation $\mu(X_{t},t)\delta$ and variance $\sigma(X_{t}, t)^{2}\delta$ and is independent of the past behavior of the process.''
\end{itemize}
Stochastic Calculus \citep{Mao2007}
\begin{itemize}
\item Integral of random process is another random process. Random process not integrable in traditional sense, so stochastic calculus created, by Ito, Ito Calculus. Use Brownian motion as some sort of reference point.
$$ Y_{t} = \int_{0}^{t} H_{s} dX_{s} $$
Integrand and integrator are stochastic processes.
\end{itemize}

\section*{March 16, 2017}
\begin{itemize}
\item Smoothness = differentiability, number of continuous derivatives (Wikipedia) 
\item Stochastic calculus = ``branch of mathematics that operates on stochastic processes. It allows a consistent theory of integration to be defined for integrals of stochastic processes with respect to stochastic processes. It is used to model systems that behave randomly.

The best-known stochastic process to which stochastic calculus is applied is the Wiener process (named in honor of Norbert Wiener), which is used for modeling Brownian motion as described by Louis Bachelier in 1900 and by Albert Einstein in 1905 and other physical diffusion processes in space of particles subject to random forces. Since the 1970s, the Wiener process has been widely applied in financial mathematics and economics to model the evolution in time of stock prices and bond interest rates.
The main flavours of stochastic calculus are the Ito calculus and its variational relative the Malliavin calculus.'' (Wikipedia) 

\item Finite element method - ``a numerical method for solving problems of engineering and mathematical physics... The finite element method formulation of the problem results in a system of algebraic equations. The {\bf method yields approximate values of the unknowns at discrete number of points over the domain. To solve the problem, it subdivides a large problem into smaller, simpler parts that are called finite elements.} The simple equations that model these finite elements are then assembled into a larger system of equations that models the entire problem. FEM then uses variational methods from the calculus of variations to approximate a solution by minimizing an associated error function.'' (Wikipedia) 
\end{itemize}

\subsection*{MEETING}
\begin{itemize}
\item Graduate this summer. Pay for three credits out of pocket. 
\item Need to email committee, see when members are available. 
\item Need to investigate the term options, see which term I will enroll for. 
\item Need to procure insurance for the summer. \\
     - Need to talk to insurance office about this 
\item April 24 Seminar: Alix is reneging on "Lessons Learned" portion \\
     - Jeffrey really impressed her with his talk, and thus Lan impressed her.  \\
     - ...so she wants to show me off, in a sense. \\
     - I gotta keep doing what I did to get to the majors. \\ 
     - ...Don't do anything different. Stick with your approach. \\
\item Alix really thinks ``so much of life is just showing up.'' 
\item Fall GTA is off the table. 
\item Alix wants me to come up with a timeline of when I will give she and Charlotte my Chapter 1, Chapter 2, Chapter 3 drafts; I agreed to middle of Spring term to provide this timeline. (deadline for the timeline) 
\end{itemize}

\section*{March 27, 2017}

\subsection*{Lecture Slides - GMRF, Dependent Spatial Data}
\citep{Lindstrom2011}
\begin{itemize}
\item Define problem, explain difficulty, define GMRF, {\bf sparse} precision matrix $Q^{-1}$, Markov property, Precision matrix construction hard
\item  Matern family:
 $$r(\pmb{u}) = \frac{\sigma^{2}}{2^{\nu - 1}\Gamma(\nu)}(\kappa||\pmb{u}||)^{\nu}K_{\nu}(\kappa||\pmb{u}||)$$
\item Random fields with Matern covariance are solutions to:
$$ (\kappa^{2} - \Delta)^{\alpha/2}x(\pmb{s}) = \mathcal{W}(\pmb{s})\text{, } $$
      \begin{itemize}
      \item $\alpha = \nu + d/2$
      \item $\Delta$ is Laplacian: $ \Delta = \sum_{i} \frac{\partial^{2}}{\partial x_{i}^{2}} $
      \end{itemize}
\end{itemize}
\subsubsection*{Construct GMRF from SPDE}
(by representing Matern field as a basis function (which is GMRF), using the SPDE identity, in what amounts to finite element method (FEM))
      \begin{itemize}
      \item Construct the solution as a finite basis expansion:
      $$ x(\pmb{s}) = \sum_{k} \psi_{k}(\pmb{s})x_{k},$$ 
      with a suitable distribution for the weights $\{x_{k}\}$.
      \item Sotchastic weak solution given by weights $\{x_{k}\}$ such that the joint distribution fulfills
      $$ \sum_{i} \left< \psi_{j}, (\kappa^{2} - \Delta)^{\alpha/2} \psi_{i} x_{i} \right> \overset{D}{=} \langle \psi_{j}, \mathcal{W} \rangle \text{  } \forall \pmb{j}$$
      \item SOLUTION. \\The distribution of the weights:
      \begin{align}
      & \pmb{x} \in N(0, \pmb{Q}^{-1}) \\
      & \alpha = 1: \pmb{Q}_{1,\kappa} = \pmb{K} \\
      & \alpha = 2: \pmb{Q}_{2,\kappa} = \pmb{K} \pmb{C}^{-1} \pmb{K}  \\
      & \alpha = 3, 4, \hdots : \pmb{Q}_{\alpha,\kappa} = \pmb{K} \pmb{C}^{-1} \pmb{Q}_{\alpha - 2,\kappa} \pmb{C}^{-1} \pmb{K},  
      \end{align}
      where
      \begin{align}
      \pmb{C}_{i,j} & = \langle \psi_{i}, \psi_{j} \rangle \\
      \pmb{K}_{i,j} & = \langle \psi_{i}, (\kappa^{2} - \Delta)\psi_{j} \rangle \\
      & = \kappa^{2} \langle \psi_{i}, \psi_{j} \rangle - \langle \psi_{i}, \Delta \psi_{j} \rangle \\
      & = \kappa^{2} \pmb{C}_{i,j} + \pmb{G}_{i,j}
      \end{align}
      \item $\pmb{C}^{-1}$ dense, so replace $\pmb{C}$ with diagonal matrix $\widetilde{\pmb{C}}_{i,i} = \langle \psi_{i}, \pmb{1} \rangle$
      \end{itemize}
\subsubsection*{Example: Lattice on $\mathbb{R}^{2}$, step size h, regular triangulation}

\begin{itemize}
\item $\pmb{Q}_{2, \kappa} = \kappa^{4} h^{2} + 2 \kappa^{2}(-\Delta) + \frac{1}{h^{2}} \Delta^{2}$
\item $\pmb{Q}_{2, \kappa} \approx \kappa^{4} h^{2} M_{0} + 2 \kappa^{2} M_{1} + \frac{1}{h^{2}} M_{2}$

\item $M_{0} = 
        \begin{bmatrix}
        &    &   &    &     \\ 
        &   &  &   &     \\ 
        &  & 1 &  &    \\ 
        &   &  &   &     \\ 
        &   &   &    & 
        \end{bmatrix}
     $
\item     $-\Delta \approx M_{1} = 
        \begin{bmatrix}
        &    &  &    &     \\ 
        &   & -1 &   &     \\ 
       & -1 & 4 & -1 &    \\ 
        &   & -1 &   &     \\ 
        &    &   &    & 
     \end{bmatrix} $
      
     
\item $ \Delta^{2} \approx M_{2} = 
        \begin{bmatrix}
        &    & 1  &    &     \\ 
        & 2  & -8 & 2  &     \\ 
      1 & -8 & 20 & -8 & 1   \\ 
        & 2  & -8 & 2  &     \\ 
        &    & 1  &    & 
     \end{bmatrix} $

\item Matrix form, matrix $\pmb{A}$:
  \begin{itemize}
  \item $ A_{i \cdot} = \big[ \psi_{1} (\pmb{u}_{i}), \dots, \psi_{N}(\pmb{u}_{i}) \big] $
  \item $x \in N(\mu, \pmb{Q}^{-1})$
  \item $\pmb{y} = \pmb{Ax} + \pmb{\epsilon}$
  \end{itemize}

\end{itemize}  

\subsection*{Stat Methods for Spatial Data Analysis} 
Matern, Whittle, Stochastic Laplace \citep{Schabenberger2004}
\begin{itemize}
\item Matern ($\theta$ range, smoothness $\nu$)
$$ C(h) = \sigma^{2}\frac{1}{\Gamma(\nu)}\left(\frac{\theta h}{2}\right)^{\nu}2K_{\nu}(\theta h) \text{,   } \nu >0, \theta > 0 $$
\item Whittle Model ($\nu = 1$) (Whittle: ``elementary model'' in $\mathbb{R}^{2}$) \\
Covariance, Correlation:
$$ C(h) = \sigma^{2}\theta h K_{1}(\theta h) $$
$$ R(h) = \theta h K_{1}(\theta h) $$
\item Stochastic Laplace equation (Z: Matern, $\epsilon$: white noise):
$$ \left( \frac{\partial^{2}}{\partial x^{2}} + \frac{\partial^{2}}{\partial y^{2}} - \theta^{2} \right) Z(x,y) = \epsilon(x,y) $$
\end{itemize}



\subsection*{Lecture Slides - GMRFs}
\citep{Lindstrom2014}

\begin{itemize}
\item Kriging
  \begin{enumerate}
  \item Observations $Y(\pmb{s}_{i}), i = 1, \hdots, n$, and unobserved locations $X(\pmb{s})$. 
  \item Simplest case, Gaussian
  $$\begin{bmatrix}
        \pmb{Y}   \\
        \pmb{X}
        \end{bmatrix}
      \sim
     N \left(
        \begin{bmatrix}
        \mu_{x}   \\
        \mu_{y}
        \end{bmatrix}
        ,
        \begin{bmatrix}
        \Sigma_{yy}     & \Sigma_{yx} \\
        \Sigma_{yx}^{T} & \Sigma_{xx}
        \end{bmatrix}
        \right) $$
  \item Parametric form
  $$ \pmb{Y} \sim N(\pmb{\mu(\theta)}, \Sigma(\pmb{\theta})) $$
  \item Log-likelihood
  $$ L(\pmb{\theta}|\pmb{Y}) = -\frac{1}{2}log|\Sigma(\pmb{\theta})| - \frac{1}{2}\Big( \pmb{Y} - \pmb{\mu}(\pmb{\theta})\Big)^{T} \Sigma(\pmb{\theta})^{-1} \Big( \pmb{Y} - \pmb{\mu}(\pmb{\theta})\Big) $$
  \item Krig
  $$ E \Big(\pmb{X}|\pmb{Y}, \hat{\theta} \Big) = \pmb{\mu}_{x} + \Sigma_{xy} \Sigma_{yy}^{-1} (\pmb{Y} - \pmb{\mu}_{y} \Big)$$
  \end{enumerate}
  
\item ``Big N'' problem
\item Getting aroung ``Big N''; lots of references
\item Gaussian Markov Random Field (GMRF) - AR1 is simplest example
\item Precision matrix $Q = \Sigma^{-1}$
\item GMRF: neighbour structure $\rightarrow$ sparse precision matrix $\rightarrow$ simplified conditional expectation
\item Brief computational details - sparse $\pmb{Q} \rightarrow$ sparse $\pmb{R}$ (Cholesky: $\pmb{R}^{T}\pmb{R} = \pmb{Q}$)
\item $\pmb{Q}$ not trivial to construct; create $\pmb{Q}$ from Matern as solution to SPDE 
\item Matern produces  Markov field for $\nu \in \mathbb{Z}$ for $\mathbb{R}^{2}$ (...via non-trivial process projection of Matern, with SPDE, onto basis representation)
\item Construct a discrete approximation of the continuous field using basis functions, $\{\psi_{k} \}$, and weights $\{ w_{k} \}$, 
$$ x(\pmb{s}) = \sum_{k} \psi_{k}(\pmb{s}) w_{k} $$
\item Find distribution of $w_{k}$ by solving 
$$ (\kappa^{2} - \Delta)^{\alpha/2}x(\pmb{s}) = \mathcal{W}(\pmb{s})$$
\item {\bf Stochastic weak solution} to the SPDE:
      $$ \left[ \left< \phi_{k}, (\kappa^{2} - \Delta)^{\alpha/2} \pmb{x} \right> \right]_{k = 1, \hdots, n} \overset{D}{=} \Big[ \langle \phi_{k}, \mathcal{W} \rangle \Big]_{k = 1, \hdots, n} $$
      ...for each set of test functions $\{\phi_{k}\}$.
\item Definition \citep{Lindgren2011}:
$$ \langle f, g \rangle = \int f(\pmb{u}) g(\pmb{u}) d\pmb{u} $$ 
\item Replace $\pmb{x}$ with basis function representation $\Sigma_{k}\psi_{k}w_{k}$
$$ \left[ \left< \phi_{i}, (\kappa^{2} - \Delta)^{\alpha/2} \psi_{j} \right> \right]_{i,j}\pmb{w} \overset{D}{=} \Big[ \langle \phi_{k}, \mathcal{W} \rangle \Big]_{k} $$
\item Galerkin solution: $\alpha = 2, \phi_{i} = \psi_{i}$
$$ \Big(
\kappa^{2} [ \langle \psi_{i}, \psi_{j} \rangle ] + [ \langle \psi_{i}, -\Delta \psi_{j} \rangle ]
\Big) \pmb{w} \overset{D}{=} \Big[ \langle \psi_{k}, \mathcal{W} \rangle \Big] $$
$$ \left(
\kappa^{2} \pmb{C} + \pmb{G} \right) \pmb{w} \overset{D}{=} N(0,\pmb{C}) $$
$$ \left(
\kappa^{2} \pmb{C} + \pmb{G} \right) \pmb{w} \sim N(0,\pmb{C}) $$

Variance(w)
\begin{align}
Var \big[ \left(\kappa^{2} \pmb{C} + \pmb{G} \right) \pmb{w} \big] & = \pmb{C} \\
\left( \kappa^{2} \pmb{C} + \pmb{G} \right) Var(\pmb{w}) \left( \kappa^{2} \pmb{C} + \pmb{G} \right)^{T} & = \pmb{C} \\
Var(\pmb{w}) & = \left( \kappa^{2} \pmb{C} + \pmb{G} \right)^{-1} \pmb{C} \left( \left( \kappa^{2} \pmb{C} + \pmb{G} \right)^{T} \right)^{-1} \\
\pmb{Q}^{-1} & = \left( \kappa^{2} \pmb{C} + \pmb{G} \right)^{-1} \pmb{C} \left( \left( \kappa^{2} \pmb{C} + \pmb{G} \right)^{T} \right)^{-1} \\
\pmb{Q} & = \left( \kappa^{2} \pmb{C} + \pmb{G} \right)^{T} \pmb{C}^{-1} \left( \kappa^{2} \pmb{C} + \pmb{G} \right)  
\end{align}
\item {\bf Piecewise linear basis function gives (almost) GMRF} \citep{Lindgren2011}. $\pmb{G}_{ij}$ and $\pmb{C}_{ij}$ sparse, but $\pmb{C}_{ij}^{-1}$ not. Replace $\pmb{C}$ with diagonal matrix $\widetilde{\pmb{C}}$
$$ \widetilde{\pmb{C}}_{i,i} = \int \psi_{i}(\pmb{s}) d\pmb{s} $$
\item Regular lattice in $\mathbb{R}^{2}$, order $\alpha = 2 (\nu = 1)$, same $M_{0}, M_{1}, M_{2}$ as above.
\item $\pmb{A}$ matrix, with rows $\pmb{A}_{i\cdot} = [\psi_{1}(\pmb{s}_{i}), \dots, \psi_{N}(\pmb{s}_{i}) ] \rightarrow \pmb{x}(\pmb{s}) = \pmb{A}(\pmb{s})\pmb{w}$ where $\pmb{w} \sim N(\mu, \pmb{Q}^{-1})$
\item Observations, kriging $E(w|y)$ (I have a logistic link between y and w, so kriging estimates not available)
\item For me: $\pmb{\eta} = \pmb{X\beta} + \pmb{Aw}$ (X = covariates, random effect Z = Aw)
\end{itemize}

\subsubsection*{Bayesian Hierarchical Model using GMRF}
\begin{itemize}
\item (1) $p(y|\eta, \theta)$, (2) $p(\eta|\theta)$, (3) $p(\theta)$ (they use X for ``latent field,'', whereas I have $\eta$ and $Z$)
\item For INLA require: $p(\pmb{y}|\pmb{x}, \pmb{\theta}) = \prod_{i} p(y_{i}|x_{i},\theta) $ 
\item Interested in:
  \begin{enumerate}
  \item Posterior for parameters: $p(\theta|\pmb{y}) \propto p(\pmb{y}|\pmb{\theta})p(\pmb{\theta})$
  \item Posterior for latent field $p(\pmb{x} | \pmb{y}, \pmb{\theta}) = \int p(\pmb{x} | \pmb{y}) p(\pmb{\theta}|\pmb{y}) d\theta$
  \end{enumerate}
\item Stats 101: $p(\pmb{y} | \pmb{x}, \pmb{\theta}) p(\pmb{x} | \pmb{\theta}) =
p(\pmb{y} , \pmb{x} | \pmb{\theta}) = p(\pmb{x} | \pmb{y}, \pmb{\theta}) p(\pmb{y} | \pmb{\theta})$
$$\rightarrow p(\pmb{y} | \pmb{\theta}) = \frac{p(\pmb{y} | \pmb{x}, \pmb{\theta}) p(\pmb{x} | \pmb{\theta})}{p(\pmb{x} | \pmb{y}, \pmb{\theta})}$$

\item (from (1) above) $$p(\theta|\pmb{y}) \propto p(\pmb{y}|\pmb{\theta})p(\pmb{\theta}) = \frac{p(\pmb{y} | \pmb{x}, \pmb{\theta}) p(\pmb{x} | \pmb{\theta})}{p(\pmb{x} | \pmb{y}, \pmb{\theta})} \cdot p(\pmb{\theta})$$ Denominator is challenge

\item Stats 101: $p(\pmb{y} | \pmb{x}, \pmb{\theta}) p(\pmb{x} | \pmb{\theta}) =
p(\pmb{y} , \pmb{x} | \pmb{\theta}) = p(\pmb{x} | \pmb{y}, \pmb{\theta}) p(\pmb{y} | \pmb{\theta})$ 

Cut out middle man:
$$p(\pmb{y} | \pmb{x}, \pmb{\theta}) p(\pmb{x} | \pmb{\theta}) = p(\pmb{x} | \pmb{y}, \pmb{\theta}) p(\pmb{y} | \pmb{\theta})$$
Divide both sides:
$$p(\pmb{y} | \pmb{x}, \pmb{\theta}) p(\pmb{x} | \pmb{\theta})/ p(\pmb{y} | \pmb{\theta})=  p(\pmb{x} | \pmb{y}, \pmb{\theta})$$
y and $\theta$ constants so proportionality:
$$p(\pmb{y} | \pmb{x}, \pmb{\theta}) p(\pmb{x} | \pmb{\theta}) \propto  p(\pmb{x} | \pmb{y}, \pmb{\theta})$$
Log of both sides:
$$\text{log } p(\pmb{x} | \pmb{y}, \pmb{\theta}) = \text{log } p(\pmb{y} | \pmb{x}, \pmb{\theta}) + \text{log } p(\pmb{x} | \pmb{\theta}) + \text{constant}$$
\item Second order Taylor approximation of $f(x) = \text{log } p(\pmb{y} | \pmb{x}, \pmb{\theta})$ about $x_{0}$
\item Obtain Gaussian approximation $p_{G}(\pmb{x} | \pmb{y}, \pmb{\theta})$:
$$ E_{x_{0}}(\pmb{x|y,\theta}) \approx \dots $$
$$  V_{x_{0}}(\pmb{x|y,\theta}) \approx \dots $$
\end{itemize}
\subsubsection*{Integrated Nested (Taylor) Laplace Approxmiation}
Evaluate $p(\theta|\pmb{y})$:
\begin{enumerate}
\item For given $\theta$ find mode: $x_{0} = \text{argmax}_{x}(\pmb{x} | \pmb{y}, \pmb{\theta})$ using $p(\pmb{x} | \pmb{y}, \pmb{\theta})$ derived from Stats 101.
\item Compute Taylor expansion of $f(x) = \text{log } p(\pmb{y} | \pmb{x}, \pmb{\theta})$ about $x_{0}$
\item Approximation:
$$ p(\theta|\pmb{y}) \approx \tilde{p}(\theta|\pmb{y}) \propto  \frac{p(\pmb{y} | \pmb{x}_{0}, \pmb{\theta}) p(\pmb{x}_{0} | \pmb{\theta})}{p_{G}(\pmb{x}_{0} | \pmb{y}, \pmb{\theta})} \cdot p(\pmb{\theta})$$
\item (Approximate) MLE is: $\hat{\theta}_{\text{ML}} \approx \text{argmax}_{\theta} \tilde{p}(\theta|\pmb{y})$
\end{enumerate}
Posteriors for $[x | y]$, use numerical integration over $\theta$:
$$ p(x_{i}|\pmb{y}) = \int p(x_{i}|\theta, \pmb{y})p(\theta|\pmb{y})d\theta \approx \sum_{k} p_{G}(\pmb{x}_{0} | \theta_{k}, \pmb{y}) \tilde{p}(\theta_{k}|\pmb{y})$$



\subsection*{Lecture Slides - Latent Gaussian Processes and SPDEs}
\citep{Lindstrom2016}
\begin{itemize}
\item More of the same. Thank god.
\end{itemize}

\subsection*{Approximate Bayesian Inference for Hierarchical Gaussian Markov Random Field Models \citep{Rue2007}}
\begin{itemize}
\item $\mathcal{O}$ definition, Wikipedia
$$f(x) = \mathcal{O}(g(x)) \text{ as } x \rightarrow \inf \text{ iff } \exists M, x_{0} \text{ such that } |f(x)| \leq M|g(x)| \text{ for all } x \geq x_{0}$$
\item Gaussian Markov Random Field 
  \begin{itemize}
  \item $\pmb{x} = \{ x_{i}:i \in \mathscr{V} \}$
  \item ``$\pmb{x}$ is a $n = |\mathscr{V}|$-dimensional Gaussian random vector with additional conditional independence/Markov properties'' 
  \item $\mathscr{V} = \{1,\dots,n\}$
  \item Graph $\mathscr{G} = \{ \mathscr{V}, \mathscr{E} \}$, with vertices, edges.
  \item ``Two nodes, $x_{i}$ and $x_{j}$ are conditionally independent given the remaining elements of $\pmb{x}$, if and only if $\{i, j\} \notin \mathscr{E}$.
  \item ``Then $\pmb{x}$ is a GMRF with respect to $\mathscr{G}$.''
  \item ``The edges in $\mathscr{E}$ are in one-to-one correspondence with the non-zero elements of the precision matrix of $\pmb{x}$, $\pmb{Q}$, in the sense that $\{ i, j \} \in \mathscr{E}$ if and only if $Q_{ij} \neq 0 \text{ for } i \neq j$.''
  \item ``When $\{i,j\} \in \mathscr{E}$ we say that $i$ and $j$ are neighbours, which we denote $i \sim j$.''
  \end{itemize}
  
\item ``A hierarchical model:
  \begin{itemize}
  \item {\bf First stage}: distributional assumptions for observables conditional on latent parameters. Given observational model parameters, often assume observations conditionally independent. 
  \item {\bf Second stage}: prior model for latent parameters, or (link) function of them. At this stage {\bf GMRFs provide a flexible tool to model the dependence} between the latent parameters and thus, implicitly, the dependence between the observed data. 
  \item {\bf Third stage}: prior distributions for unknown hyperparameters (precision parameters in the GMRF).''
  \end{itemize}
\item ``We propose a deterministic alternative to MCMC based inference... computed almost instant[ly]... proves to be quite accurate.''
\end{itemize}

\subsection*{INLA} % *********************************************
\begin{enumerate}
\item $p(y|x,\theta)$ --- Easy.
\item $p(x|\theta, y)$ --- Gaussian approximation.
\item $p(\theta|y)$ --- Identity, Gaussian approximation, evaluate at $x_{0}$
\item $p(x_{i}|y)$ --- Numerical integration of $p(x_{i}|\theta, y)p(\theta |y)$ over all $\theta$
\item $p(x|y)$ --- Never.
\item $p(x,\theta | y)$ --- Never.
\end{enumerate}

\subsection*{INLA}
\begin{enumerate}

\item $p(y|x,\theta)$ --- Easy.

\item $p(x|\theta, y)$ --- Gaussian approximation.
      \begin{itemize}
      \item \cite{Rue2005} (Chapter 4) - ``$\pi(x|\theta,y)$ can often be well approximated with a Gaussian distribution, by matching the mode and curvature at the mode.''
      \end{itemize}
      
\cite{Rue2007}
      \begin{itemize}
      \item $p(x|\theta,y) \propto \text{exp}\left(-\frac{1}{2}\pmb{x}^{T}\pmb{Qx} + \sum_{i} \text{log}p(y_{i}|x_{i}) \right)$ 
      \item $p_{G}(x|\theta,y) \propto \text{exp} \left( -\frac{1}{2}(\pmb{x-\mu})^{T} (\pmb{Q} + \text{diag}(\pmb{c}) ) (\pmb{x - \mu}) \right)$
                \begin{itemize}
                \item $\mu$ = mode of $p(\pmb{x}|\pmb{\theta, y}$ for each $\theta$ \citep{Rue2007}
                \item Terms of $\pmb{c}$ due to 2nd order Taylor expansion at $\mu = x_{0} =$ mode of $\sum_{i} \text{log}p(y_{i}|x_{i}) = \text{log}f(y|x)$
                \end{itemize}
      \end{itemize}
      
\citep{Lindstrom2014}
      \begin{itemize}
      \item For a given $\theta = \theta_{0}$ find the mode: $x_{0} = \text{argmax}_{x}p(\pmb{x}|\pmb{y},\pmb{\theta}_{0})$
                \begin{itemize}
                \item Can find from unnormalized $p(\pmb{x} | \pmb{y}, \pmb{\theta})$, which we have up to a proportionality constant (good enough for mode)
                \item $p_{G}(\cdot)$ is valid pdf
                \end{itemize}
      \item $\text{log } p(\pmb{x} | \pmb{y}, \pmb{\theta}) = \text{log } p(\pmb{y} | \pmb{x}, \pmb{\theta}) + \text{log } p(\pmb{x} | \pmb{\theta}) + \text{constant}$
      \item Second order Taylor approximation of $f(x) = \text{log } p(\pmb{y} | \pmb{x}, \pmb{\theta})$ about $x_{0}$
      \item Obtain Gaussian approximation $p_{G}(\pmb{x} | \pmb{y}, \pmb{\theta})$:
            \begin{align}
            E_{x_{0}}(\pmb{x|y,\theta}) & \approx \big( \pmb{Q} - \text{diag}(f''(x_{0})) \big)^{-1} \big( \pmb{Q\mu} + f'(x_{0}) - f''(x_{0})x_{0} \big)  \\
            V_{x_{0}}(\pmb{x|y,\theta}) & \approx \big( \pmb{Q} - \text{diag}(f''(x_{0})) \big)^{-1} 
            \end{align}
      \end{itemize}
  
\item $p(\theta|y)$ --- Identity, mode of x for $\theta_{0}$, Gaussian approximation, $
\theta_{ML} = \text{argmax}_{\theta} p(\theta|y)$
      \begin{itemize}
      \item Ends up begin same as \cite{Tierney1986} Laplace Approximation
      \item Stats 101: $p(\pmb{y} | \pmb{x}, \pmb{\theta}) p(\pmb{x} | \pmb{\theta}) =
p(\pmb{y} , \pmb{x} | \pmb{\theta}) = p(\pmb{x} | \pmb{y}, \pmb{\theta}) p(\pmb{y} | \pmb{\theta})$

      \item Bayesian 101, and identity: $$p(\theta|\pmb{y}) \propto p(\pmb{y}|\pmb{\theta})p(\pmb{\theta}) = \frac{p(\pmb{y} | \pmb{x}, \pmb{\theta}) p(\pmb{x} | \pmb{\theta})}{p(\pmb{x} | \pmb{y}, \pmb{\theta})} \cdot p(\pmb{\theta})$$
      \item As above: For a given $\theta = \theta_{0}$ find the mode: $x_{0} = \text{argmax}_{x}p(\pmb{x}|\pmb{y},\pmb{\theta}_{0})$ 
      \item Gaussian approximation $p_{G}(\pmb{x} | \pmb{y}, \pmb{\theta})$ from above.
      \item Approximation of $p(\theta|y)$:
      $$\tilde{p}(\theta|\pmb{y}) \propto  \frac{p(\pmb{y} | \pmb{x}_{0}, \pmb{\theta}) p(\pmb{x}_{0} | \pmb{\theta})p(\pmb{\theta})}{p_{G}(\pmb{x}_{0} | \pmb{y}, \pmb{\theta})}$$
      \item $\theta_{ML} = \text{argmax}_{\theta} \tilde{p}(\theta|y)$
      \end{itemize}
\item $p(x_{i}|y)$ --- Numerical integration of $p(x_{i}|\theta, y)p(\theta |y)$ over all $\theta$
      \begin{itemize}
      \item Posteriors for $[x | y]$, use numerical integration over $\theta$:
$$ p(x_{i}|\pmb{y}) = \int p(x_{i}|\theta, \pmb{y})p(\theta|\pmb{y})d\theta \approx \sum_{k} p_{G}(\pmb{x}_{i} | \theta_{k}, \pmb{y}) \tilde{p}(\theta_{k}|\pmb{y})$$
      \end{itemize}
\item $p(x|y)$ --- Never.
\item $p(x,\theta | y)$ --- Never.

\end{enumerate}

\subsection*{Think continuous: Markovian Gaussian models in spatial statistics }
\citep{Simpson2012}
\begin{itemize}
\item {\bf 3.4. Approximating Gaussian random fields: the finite element method)}
\item
\end{itemize}

\section{April 3, 2017}
SPDE-INLA summary: A fancy approximation technique called INLA works well for Bayesian hierarchical models with latent Gaussian markov random fields, which are discrete, and have a sparse precision matrix---which is essential. To make this technique work for continuous domain spatial data and continuous spatial latent gaussian random fields, a fancy identity comprised of a linear fractional stochastic partial differential equation helpfully relates a (continuous) Matern random field to Gaussian random field white noise process. This identity undergirds approximating the Matern with a piecewise linear basis representation, with deterministic basis functions and GMRF weights. The method covers the domain with a triangular mesh to define the piecewise linear basis function representation. This process, used extensively in other fields, is known as the Finite Element Method. The basis representation has a sparse precision matrix, and enables INLA approximation techniques that offer so many computational advantages. 

\subsection{Problems}
\begin{itemize}
\item Points are too fucking close together. Unnecessarily close together. I can barely generate Matern data (n = 3000) with \verb|grf(...)| because the matrix operations crash.
\item Solution: use Var-res grid structure to discretize to binomial, then INLA procedure.
\item The \verb|sd()| of Matern fields generated by \verb|grf()| is nowhere near nominal value for n=500 (might be okay, on account of effects of correlation)
\end{itemize}

\subsection{Meeting}
\begin{itemize}
\item
\item
\end{itemize}



\subsection*{As of Now}
\begin{itemize}
\item INLA, SPDE, R-INLA
\item Jobs
  \begin{itemize}
  \item Post docs
  \item Talk to Alix, Charlotte
  \item Jaime, MIT/Cambridge John
  \item Apply to junior quant/tech positions
  \end{itemize}
\item Writing
\item Python (Code Academy), Github, Machine Learning (Dummies), C++ (Dummies)
\item
\item Reschedule defense. Replace Lisa?
\item Graduate/commencement: \\
(1) http://gradschool.oregonstate.edu/progress/commencement \\ 
(2) http://gradschool.oregonstate.edu/progress/deadlines \\
\item 
\item Scoring rules - model evaluation/comparison (two papers)
\item Spatial design analysis - knot selection (two papers)
\item Convergence diagnostics (CODA package, Finley paper)
\item Simulate, or real data?
\end{itemize}

% \bibliographystyle{plainnat}
\bibliography{Baseball}

\end{document}