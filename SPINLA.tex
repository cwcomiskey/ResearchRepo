\documentclass{article}

\title{Take Me Out to (Analyze) the Ballgame}
\author{Chris Comiskey}
\date{\today}

\usepackage{natbib}
\bibliographystyle{unsrtnat}

\usepackage{fullpage}
\usepackage{ulem}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{mathtools}
\usepackage{float}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[export]{adjustbox}

\usepackage{mathrsfs}


\makeatletter
\def\verbatim@font{\linespread{1}\normalfont\ttfamily}
\makeatother

\usepackage[toc,page]{appendix}


% \setlength{\parindent}{4em}
% \setlength{\parskip}{1em}
% \renewcommand{\baselinestretch}{1.3}

\usepackage{setspace}
\doublespacing

\begin{document}
\maketitle{}

\tableofcontents




\section{Method 3 - INLA and SPDEs} % =================



\subsection{Matern Covariance}

\subsection{Integrated Nested Laplace Approximations (INLA)}

\subsubsection{Gaussian Markov Random Fields}

Approximate Bayesian Inference for Hierarchical Gaussian Markov Random Field Models \citep{Rue2007}
\begin{itemize}
\item $\mathcal{O}$ definition, Wikipedia
$$f(x) = \mathcal{O}(g(x)) \text{ as } x \rightarrow \inf \text{ iff } \exists M, x_{0} \text{ such that } |f(x)| \leq M|g(x)| \text{ for all } x \geq x_{0}$$
\item Gaussian Markov Random Field 
   \begin{itemize}
   \item $\pmb{x} = \{ x_{i}:i \in \mathscr{V} \}$
   \item ``$\pmb{x}$ is a $n = |\mathscr{V}|$-dimensional Gaussian random vector with additional conditional independence/Markov properties''
   \item $\mathscr{V} = \{1,\dots,n\}$
   \item Graph $\mathscr{G} = \{ \mathscr{V}, \mathscr{E} \}$, with vertices, edges.
   \item ``Two nodes, $x_{i}$ and $x_{j}$ are conditionally independent given the remaining elements of $\pmb{x}$, if and only if $\{i, j\} \notin \mathscr{E}$.
   \item ``Then $\pmb{x}$ is a GMRF with respect to $\mathscr{G}$.''
   \item ``The edges in $\mathscr{E}$ are in one-to-one correspondence with the non-zero elements of the precision matrix of $\pmb{x}$, $\pmb{Q}$, in the sense that $\{ i, j \} \in \mathscr{E}$ if and only if $Q_{ij} \neq 0 \text{ for } i \neq j$.''
   \item ``When $\{i,j\} \in \mathscr{E}$ we say that $i$ and $j$ are neighbours, which we denote $i \sim j$.''
   \end{itemize}

 \item Nice destription of hierarchical model architecture... ``One of the main areas of application for GMRFs is that of (Bayesian) hierarchical models. A hierarchical model is characterised by {\bf several stages} of observables and parameters. The {\bf first stage}, typically, consists of distributional assumptions for the observables conditionally on latent parameters. For example if we observe a time series of counts y, we may assume, for $y_{i}$, $i \in D \subset R^{?}$ a Poisson distribution with unknown mean  $\lambda_{i}$. Given the parameters of the observation model, we often assume the observations to be conditionally independent. The {\bf second stage} consists of a prior model for the latent parameters $\lambda_{i}$ or, more often, for a particular function of them. For example, in the Poisson case we can choose an exponential link and model the random variables $x_{i} = log(\lambda_{i})$. At this stage {\bf GMRFs provide a flexible tool to model the dependence} between the latent parameters and thus, implicitly, the dependence between the observed data. This dependence can be of various kind, such as temporal, spatial, or even spatiotemporal. The {\bf third stage} consists of prior distributions for the unknown hyperparameters  . These are typically precision parameters in the GMRF.''
 \item ``We propose a deterministic alternative to MCMC based inference... computed almost instant[ly]... proves to be quite accurate.''
 \item Oops. {\bf Gaussian Markov Random Field $\neq$ Gaussian Random Field}; \\
 GMRF has different properties; think graph theory edges, and non-edge-connected nodes have zero correlation, hence sparse matrices for INLA
\end{itemize}

\subsubsection{INLA}


Approximate Bayesian Inference for Hierarchical Gaussian Markov Random Field Models \citep{Rue2007} ...More to this paper, on INLA

\subsection{Stochastic Partial Differential Equations (SPDE)}

\subsubsection{Stochastic Calculus}

Stochastic Differential Equation (wikipedia)
\begin{itemize}
\item ``A heuristic but helpful interpretation of the stochastic differential equation (of continuous time stochastic process $X_{t}$) is that in a small time interval of length $\delta$ the stochastic process $X_{t}$ changes its value by an amount that is normally distributed (for example) with expectation $\mu(X_{t},t)\delta$ and variance $\sigma(X_{t}, t)^{2}\delta$ and is independent of the past behavior of the process.''
\end{itemize}
Stochastic Calculus \citep{Mao2007}
\begin{itemize}
\item Integral of random process is another random process. Random process not integrable in traditional sense, so stochastic calculus created, by Ito, Ito Calculus. Use Brownian motion as some sort of reference point.
$$ Y_{t} = \int_{0}^{t} H_{s} dX_{s} $$
Integrand and integrator are stochastic processes.
\end{itemize}

\subsubsection{Linking INLA and SPDEs}

An explicit link between Gaussian fields and Gaussian Markov random fields: the stochastic partial differential equation approach \citep{Lindgren2011}
\begin{itemize}
\item ``A GMRF is a discretely indexed Gaussian field x, where the full conditionals'' depend only on a small set of symmetric neighbor relationships, which yields sparse matrices that lend themselves to approximations---INLA
\item The GMRF ``computational gain comes from the fact that the zero pattern of the precision matrix Q (the inverse covariance matrix) relates directly to the notion of neighbours...''
\item Matrix {\bf Q: the inverse covariance matrix}
\item ``The result is a basis function representation with piecewise linear basis functions, and Gaussian weights with Markov dependencies determined by a general triangulation of the domain.'' \citep{Lindgren2011}
  \begin{itemize}
  \item Recall ``basis'' from linear algebra, where a set of linearly independent vectors span a space
  \item function space - space of {\it functions}
  \item basis functions - a set of functions from which can build any function in the function space
  \end{itemize}

\item Matern (2):
$$r(\pmb{u}, \pmb{v}) = \frac{\sigma^{2}}{2^{\nu - 1}\Gamma(\nu)}(\kappa||\pmb{u} - \pmb{v}||)^{\nu}K_{\nu}(\kappa||\pmb{u} - \pmb{v}||)$$
      \begin{itemize}
      \item scaling parameter $\kappa = \rho$ range parameter
      \item Empirically derived: $\rho = \sqrt{8\nu}/\kappa$
      \end{itemize}
\item Linear fractional SPDE:
$$ (\kappa^{2} - \Delta)^{\alpha/2} x(\pmb{u}) = \mathcal{W}(\pmb{u}) $$
      \begin{itemize}
      \item $\alpha = \nu + d/2$
      \end{itemize}
\item $\Delta$ is Laplacian:
$$ \Delta = \sum_{i=1}^{d} \frac{\partial^{2}}{\partial x_{i}^{2}} $$
\item Marginal Variance:
$$\sigma^{2} = \frac{\Gamma(\nu)}{\Gamma(\nu + d/2)(4\pi)^{d/2}\kappa^{2\nu}}$$
\end{itemize}

\subsection{Bayesian Inference with INLA and SPDEs: R-INLA}

Bayesian Spatial Modelling with R-INLA \citep{Lindgren2015}
\begin{itemize}
\item ``...as discussed in Lindgren et al. (2011), one can express a large class of random field models as solutions to continuous domain stochastic partial differential equations (SPDEs), and write down explicit links between the parameters of each SPDE and the elements of precision matrices for weights in a discrete basis function representation.''
\item ``As discussed in the introduction, {\bf an alternative to traditional covariance based modelling is to use SPDEs}, but carry out the practical computations using Gaussian Markov random field (GMRF) representations. This is done by {\bf approximating the full set of spatial random functions with weighted sums of simple basis functions}, which allows us to hold on to the continuous interpretation of space, while the {\bf computational algorithms} only see discrete structures with Markov properties. Beyond the main paper Lindgren et al. (2011), this is further discussed by Simpson, Lindgren, and Rue (2012a,b).''
\item ``The simplest model for (spatial field) $x(\pmb{s})$ currently implemented in R-INLA is the SPDE/GMRF version of the stationary Matern family, obtained as the stationary solutions to
$$ (\kappa^{2} - \Delta)^{\alpha/2}(\tau x(\pmb{s})) = \mathcal{W}(\pmb{s})\text{, } \pmb{s} \in \Omega $$ where
    \begin{itemize}
    \item $\Delta$ is the Laplacian
    \item $\kappa$ is the spatial scale parameter
    \item $\alpha$ controls the smoothness of the realisations
    \item $\tau$ controls the variance
    \item $\Omega$ is the spatial domain
    \item $\mathcal{W}(\pmb{s})$ is a Gaussian spatial white noise process
    \end{itemize}
Whittle (1954, 1963) shows stationary solutions on $\mathbb{R}^{d}$ have Matern covariances,
$$\text{COV}(x(\pmb{0}), x(\pmb{s})) = \frac{\sigma^{2}}{2^{\nu - 1}\Gamma(\nu)}(\kappa||s||)^{\nu}K_{\nu}(\kappa||s||)$$
The parameters in the two formulations are coupled so that the Matern smoothness is $\nu = \alpha - d/2$ and marginal variance is
$$\sigma^{2} = \frac{\Gamma(\nu)}{\Gamma(\alpha)(4\pi)^{d/2}\kappa^{2\nu}\tau^{2}}$$
Exponential covariance: $\nu = 1/2$; (i) for $d = 1 \rightarrow  \alpha = 1$, (ii) for $d = 2 \rightarrow \alpha = 3/2$''
\item ``The models discussed in \cite{Lindgren2011} and implemented in R-INLA are built on a basis representation
$$ x(\pmb{s}) = \sum_{k=1}^{n} \psi_{k}(\pmb{s})x_{k}$$
where
      \begin{itemize}
      \item $\psi_{k}(\cdot)$ are deterministic basis functions, and
      \item the joint distribution of the weight vector $\pmb{x} = \{x_{1},\dots,x_{n}\}$ is chosen so that the distribution of the functions $x(\pmb{s})$ approximates the distribution of solutions to the SPDE on the domain.''
      \item piecewise polynomial basis functions
      \item use Finite Element Method - project the SPDE onto the basis representation 
      \end{itemize}
\item 
\end{itemize}

\appendix
\section{Blah Blah Blah} % =================

\bibliography{Baseball}

\end{document}